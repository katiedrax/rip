
@article{ ISI:000534851200005,
Author = {Tan, Zheyu and Beuran, Razvan and Hasegawa, Shinobu and Jiang, Weiwei
   and Zhao, Min and Tan, Yasuo},
Title = {{Adaptive security awareness training using linked open data datasets}},
Journal = {{EDUCATION AND INFORMATION TECHNOLOGIES}},
Year = {{2020}},
Volume = {{25}},
Number = {{6}},
Pages = {{5235-5259}},
Month = {{NOV}},
Abstract = {{Cybersecurity is no longer an issue discussed only between the
   professionals or technologists, but it is also closely related to
   ordinary people whose daily life is exposed to kinds of cyberattacks.
   And Womabat Security Technologies conducted a survey revealed that
   ransomware is an unknown concept to nearly two-thirds of employees. In
   practical, almost 95\% of cybersecurity attacks are due to human error.
   At fact, expensive and sophisticated systems cannot work effectively
   without considering the human factor, while human factor is the major
   vulnerability in cybersecurity. Thus, it has great significance to give
   people cybersecurity awareness training. In this paper, we present a
   system, named ASURA, providing adaptive training aimed at improving
   cybersecurity awareness of people. Three issues can't be neglected in
   adaptive cybersecurity awareness training, as follows. Firstly, we need
   to decide the proper training contents from the huge training materials.
   Secondly, the training contents should be timely updated, as cyber
   attacks constantly changing. At last, we should conduct training through
   effective and acceptable approach. We solved above three issues in this
   paper, and the innovative idea of this paper is constructing
   hierarchical concept map from the LOD database DBpedia. Then, we employ
   a series of processing on hierarchical concept map, including PageRank
   algorithm used to calculate the importance of each concept node, and
   filtering used to filtered out undefined and unrelated concepts. In
   particular, we get training contents from DBpedia dynamically and timely
   updated, so that training contents is keeping up to date. ASURA
   delivered training contents completely online, thus significant trimmed
   budget and allowed learners accessing training outside of a traditional
   classroom. Moreover, ASURA provide adaptive training targeted to
   individual learner, as it generate training contents based on the
   keyword from the learner.}},
DOI = {{10.1007/s10639-020-10155-x}},
Early Access Date = {{MAY 2020}},
ISSN = {{1360-2357}},
EISSN = {{1573-7608}},
Unique-ID = {{ISI:000534851200005}},
}

@article{ ISI:000648694200007,
Author = {Yin, Lihua and Feng, Jiyuan and Lin, Sixin and Cao, Zhiqiang and Sun,
   Zhe},
Title = {{A blockchain-based collaborative training method for multi-party data
   sharing}},
Journal = {{COMPUTER COMMUNICATIONS}},
Year = {{2021}},
Volume = {{173}},
Pages = {{70-78}},
Month = {{MAY 1}},
Abstract = {{In recent years, the construction of Space-Ground Integrated Network has
   been accelerated, connecting different types of networks in remote
   regions. The various devices are connected together, so that data that
   was difficult to communicate before can be used to train particular
   models together, giving birth to new service models. Privacy issues,
   however, remain a substantial concern affecting data sharing among
   multiple parties. Cooperative training methods such as federated
   learning usually require a centralized aggregator to aggregate the
   dispersed sub-models. In general, various privacy-preserving methods
   assume the aggregator as an honest-but-curious (HBC) role and cannot
   guarantee that the program can be executed correctly. In this paper, we
   propose a blockchain-based collaborative training method that uses the
   decentralized accounting technology of the blockchain to solve the trust
   problem between different participants. Through the anti repudiation
   nature of the blockchain, it is ensured that the aggregation of the
   model is executed correctly. We designed a function encryption-based
   privacy preserving method in which the aggregator can only obtain the
   results of the aggregation model, and cannot access the models uploaded
   to the blockchain from other participants. Subsequently, a prototype
   system based on blockchain is developed to analyze and evaluate the time
   consumption of our proposed cooperative training method and function
   encryption module. The result of our experiments shows the feasibility
   of our cooperative training model.}},
DOI = {{10.1016/j.comcom.2021.03.027}},
ISSN = {{0140-3664}},
EISSN = {{1873-703X}},
Unique-ID = {{ISI:000648694200007}},
}

@article{ ISI:000587684400001,
Author = {del Carmen Galvez-de-la-Cuesta, Maria and Gertrudix-Barrio, Manuel and
   Garcia-Garcia, Francisco},
Title = {{Open Data and Education: Teacher Training in the Digital Society}},
Journal = {{PAGINAS DE EDUCACION}},
Year = {{2020}},
Volume = {{13}},
Number = {{2}},
Pages = {{1-20}},
Month = {{JUL-DEC}},
Abstract = {{The educational environment faces an inevitable adaptation to the
   digital environment, where the uncritical consumption of information by
   students is constant. Ensuring critical consumption requires
   intensifying the processes of media and information literacy, linked to
   the digital competence of teachers, which can be reinforced by the use
   of multimedia products based on open data. The purpose of this research
   is to analyze the degree of digital literacy and media education that
   future teachers of Primary Education possess. A case study methodology
   was adopted, through semi-structured interviews with the teachers in
   charge of training future teachers in Primary Education at the
   University of Castilla la Mancha (Spain). The interpretation of results
   was made using the content analysis technique, with an exhaustive review
   model. The need to promote media and information literacy in the field
   of teacher training is concluded.}},
DOI = {{10.22235/pe.v13i2.1913}},
ISSN = {{1688-5287}},
EISSN = {{1688-7468}},
ResearcherID-Numbers = {{de la Cuesta, Maria del Carmen Galvez/J-2204-2019
   }},
ORCID-Numbers = {{de la Cuesta, Maria del Carmen Galvez/0000-0002-0208-4311
   Gertrudix Barrio, Manuel/0000-0002-5869-3116}},
Unique-ID = {{ISI:000587684400001}},
}

@article{ ISI:000407661100007,
Author = {Rocchini, Duccio and Petras, Vaclav and Petrasova, Anna and Horning, Ned
   and Furtkevicova, Ludmila and Neteler, Markus and Leutner, Benjamin and
   Wegmann, Martin},
Title = {{Open data and open source for remote sensing training in ecology}},
Journal = {{ECOLOGICAL INFORMATICS}},
Year = {{2017}},
Volume = {{40}},
Pages = {{57-61}},
Month = {{JUL}},
Abstract = {{Remote sensing is one of the most important tools in ecology and
   conservation for an effective monitoring of ecosystems in space and
   time. Hence, a proper training is crucial for developing effective
   conservation practices based on remote sensing data. In this paper we
   aim to highlight the potential of open access data and open source
   software and the importance of the inter-linkages between these and
   remote sensing training, with an interdisciplinary perspective. We will
   first deal with the importance of open access data and then we provide
   several examples of Free and Open Source Software (FOSS) for a deeper
   and more critical understanding of its application in remote sensing.}},
DOI = {{10.1016/j.ecoinf.2017.05.004}},
ISSN = {{1574-9541}},
EISSN = {{1878-0512}},
ResearcherID-Numbers = {{Neteler, Markus/C-6328-2008
   }},
ORCID-Numbers = {{Neteler, Markus/0000-0003-1916-1966
   Petras, Vaclav/0000-0001-5566-9236
   Petrasova, Anna/0000-0002-5120-5538}},
Unique-ID = {{ISI:000407661100007}},
}

@inproceedings{ ISI:000296062405151,
Author = {Mandal, Arindam and Vergyri, Dimitra and Akbacak, Murat and Richey,
   Colleen and Kathol, Andreas},
Book-Group-Author = {{IEEE}},
Title = {{ACOUSTIC DATA SHARING FOR AFGHAN AND PERSIAN LANGUAGES}},
Booktitle = {{2011 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL
   PROCESSING}},
Series = {{International Conference on Acoustics Speech and Signal Processing
   ICASSP}},
Year = {{2011}},
Pages = {{4996-4999}},
Note = {{IEEE International Conference on Acoustics, Speech, and Signal
   Processing (ICASSP), Prague Congress Ctr, Prague, CZECH REPUBLIC, MAY
   22-27, 2011}},
Organization = {{Inst Elect \& Elect Engineers Signal Processing Soc; IEEE}},
Abstract = {{In this work, we compare several known approaches for multilingual
   acoustic modeling for three languages, Dari, Farsi and Pashto, which are
   of recent geo-political interest. We demonstrate that we can train a
   single multilingual acoustic model for these languages and achieve
   recognition accuracy close to that of monolingual (or
   language-dependent) models. When only a small amount of training data is
   available for each of these languages, the multilingual model may even
   outperform the monolingual ones. We also explore adapting the
   multilingual model to target language data, which are able to achieve
   improved automatic speech recognition (ASR) performance compared to the
   monolingual models for both large and small amounts of training data by
   3\% relative word error rate (WER).}},
ISSN = {{1520-6149}},
ISBN = {{978-1-4577-0539-7}},
Unique-ID = {{ISI:000296062405151}},
}

@article{ ISI:000371105400011,
Author = {Hazlett, Denice Rovira},
Title = {{Libraries Develop Open Data Training Program}},
Journal = {{LIBRARY JOURNAL}},
Year = {{2016}},
Volume = {{141}},
Number = {{4}},
Pages = {{24+}},
Month = {{MAR 1}},
ISSN = {{0363-0277}},
Unique-ID = {{ISI:000371105400011}},
}

@article{ ISI:000637580000001,
Author = {Logan, Jessica A. R. and Hart, Sara A. and Schatschneider, Christopher},
Title = {{Data Sharing in Education Science}},
Journal = {{AERA OPEN}},
Year = {{2021}},
Volume = {{7}},
Month = {{APR}},
Abstract = {{Many research agencies are now requiring that data collected as part of
   funded projects be shared. However, the practice of data sharing in
   education sciences has lagged these funder requirements. We assert that
   this is likely because researchers generally have not been made aware of
   these requirements and of the benefits of data sharing. Furthermore,
   data sharing is usually not a part of formal training, so many
   researchers may be unaware of how to properly share their data. Finally,
   the research culture in education science is often filled with concerns
   regarding the sharing of data. In this article, we address each of these
   areas, discussing the wide range of benefits of data sharing, the many
   ways by which data can be shared; provide a step by step guide to start
   sharing data; and respond to common concerns.}},
DOI = {{10.1177/23328584211006475}},
Article-Number = {{23328584211006475}},
EISSN = {{2332-8584}},
Unique-ID = {{ISI:000637580000001}},
}

@article{ ISI:000596477400011,
Author = {Han, Tianyu and Nebelung, Sven and Haarburger, Christoph and Horse,
   Nicolas and Reinartz, Sebastian and Merhof, Dorit and Kiessling, Fabian
   and Schulzl, Vol Kma R. and Truhn, Daniel},
Title = {{Breaking medical data sharing boundaries by using synthesized
   radiographs}},
Journal = {{SCIENCE ADVANCES}},
Year = {{2020}},
Volume = {{6}},
Number = {{49}},
Month = {{DEC}},
Abstract = {{Computer vision (CV) has the potential to change medicine fundamentally.
   Expert knowledge provided by CV can enhance diagnosis. Unfortunately,
   existing algorithms often remain below expectations, as databases used
   for training are usually too small, incomplete, and heterogeneous in
   quality. Moreover, data protection is a serious obstacle to the exchange
   of data. To overcome this limitation, we propose to use generative
   models (GMs) to produce high-resolution synthetic radiographs that do
   not contain any personal identification information. Blinded analyses by
   CV and radiology experts confirmed the high similarity of synthesized
   and real radiographs. The combination of pooled GM improves the
   performance of CV algorithms trained on smaller datasets, and the
   integration of synthesized data into patient data repositories can
   compensate for underrepresented disease entities. By integrating
   federated learning strategies, even hospitals with few datasets can
   contribute to and benefit from GM training.}},
DOI = {{10.1126/sciadv.abb7973}},
Article-Number = {{eabb7973}},
ISSN = {{2375-2548}},
ResearcherID-Numbers = {{Truhn, Daniel/AAL-1950-2021
   Truhn, Daniel/AAG-9359-2021
   Kiessling, Fabian/Y-2417-2019
   Reinartz, Sebastian/S-1106-2018
   }},
ORCID-Numbers = {{Truhn, Daniel/0000-0002-9605-0728
   Reinartz, Sebastian/0000-0003-1769-4953
   Han, Tianyu/0000-0002-8636-6462
   Kiessling, Fabian/0000-0002-7341-0399}},
Unique-ID = {{ISI:000596477400011}},
}

@inproceedings{ ISI:000377252400012,
Author = {Hugi, Jasmin and Schneider, Rene},
Editor = {{Kurbanoglu, S and Spiranec, S and Grassian, E and Mizrachi, D and Catts, R}},
Title = {{Linked Open Data Literacy for Librarians}},
Booktitle = {{INFORMATION LITERACY: LIFELONG LEARNING AND DIGITAL CITIZENSHIP IN THE
   21ST CENTURY}},
Series = {{Communications in Computer and Information Science}},
Year = {{2014}},
Volume = {{492}},
Pages = {{109-117}},
Note = {{2nd European Conference on Information Literacy (ECIL), Dubrovnik,
   CROATIA, OCT 20-23, 2014}},
Organization = {{Zagreb Univ, Dept Informat \& Commun Sci; Hacettepe Univ, Dept Informat
   Management}},
Abstract = {{Linked Data has become an important issue, not only for the process of
   building the web of data, but also for the mutual knowledge transfer
   between libraries and the web of data. Based on a study concerning
   Linked Open Data applications in libraries and the qualifications of
   future librarians involved in the development of such applications, we
   built a one day training program for academic librarians. The overall
   goal was to make librarians literate and heighten awareness concerning
   the Linked Open Data technology on a single day. With the help of that
   training program, the librarians should become cognitively able to
   reflect on the integration of Linked Data in their current working
   environment. We considered it therefore necessary to couple the training
   program with the development of cognitive strategies. In this paper we
   will give an in-depth description of the didactical approach, the
   theoretical and practical components as well as their possible
   combinations.}},
ISSN = {{1865-0929}},
ISBN = {{978-3-319-14136-7; 978-3-319-14135-0}},
Unique-ID = {{ISI:000377252400012}},
}

@inproceedings{ ISI:000380412500041,
Author = {Olga, Maksimenkova and Vadim, Podbelskiy},
Book-Group-Author = {{IEEE}},
Title = {{On Practice of Using Open Data in Construction of Training and
   Assessment Tasks for Programming Courses}},
Booktitle = {{10TH INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE \& EDUCATION (ICCSE
   2015)}},
Series = {{International Conference on Computer Science \& Education}},
Year = {{2015}},
Pages = {{233-236}},
Note = {{10th International Conference on Computer Science and Education (ICCSE),
   Brunel Univ London, Cambridge, ENGLAND, JUL 22-24, 2015}},
Organization = {{IEEE; CCSE; Inst Elect \& Elect Engineers, UK Sect; Natl Res Council
   Comp Educ Coll \& Univ; Cambridge Univ, Fitzwilliam Coll; Inst Elect \&
   Elect Engineers, Ireland Sect}},
Abstract = {{This paper describes the experience of using open datasets in
   ``Programming{''} course for the first-year undergraduates in Software
   engineering. The course is designed according to a combined approach. It
   contains the features of problem based learning and formative
   assessment. We describe and give examples of open datasets using in
   training and assessment task development. We also present the problems
   of open data application to educational process and possible ways of
   their solving.}},
ISSN = {{2471-6146}},
ISBN = {{978-1-4799-6600-4}},
ResearcherID-Numbers = {{Podbelskiy, Vadim/J-8606-2015
   Maksimenkova, Olga/AAC-4141-2019}},
ORCID-Numbers = {{Podbelskiy, Vadim/0000-0001-9728-3593
   Maksimenkova, Olga/0000-0003-2467-730X}},
Unique-ID = {{ISI:000380412500041}},
}

@article{ ISI:000569062000026,
Author = {Cai, Xiaoran and Mo, Xiaopeng and Chen, Junyang and Xu, Jie},
Title = {{D2D-Enabled Data Sharing for Distributed Machine Learning at Wireless
   Network Edge}},
Journal = {{IEEE WIRELESS COMMUNICATIONS LETTERS}},
Year = {{2020}},
Volume = {{9}},
Number = {{9}},
Pages = {{1457-1461}},
Month = {{SEPT}},
Abstract = {{Mobile edge learning is an emerging technique that enables distributed
   edge devices to collaborate in training shared machine learning (ML)
   models by exploiting their local data samples and
   communication/computation resources. To deal with the ``straggler's
   dilemma{''} issue faced in this technique, this letter proposes a new
   device-to-device (D2D)-enabled data sharing approach, in which different
   edge devices share their data samples among each other over D2D
   communication links, in order to properly adjust their computation loads
   for increasing the training speed. Under this setup, we optimize the
   radio resource allocation for both D2D-enabled data sharing and
   distributed training, with the objective of minimizing the total
   training delay under fixed numbers of local and global iterations (for
   training). Numerical results show that the proposed D2D-enabled data
   sharing design significantly reduces the training delay, and also
   enhances the training accuracy when the data samples are non-independent
   and identically distributed (non-IID) among edge devices.}},
DOI = {{10.1109/LWC.2020.2993837}},
ISSN = {{2162-2337}},
EISSN = {{2162-2345}},
Unique-ID = {{ISI:000569062000026}},
}

@inproceedings{ ISI:000299185000028,
Author = {Pochyla, Martin},
Editor = {{Doucek, P and Chroust, G and Oskrdal, V}},
Title = {{SHARING DATA AND INFORMATION THROUGH DIGITAL PORTFOLIO}},
Booktitle = {{IDIMT-2011: INTERDISCIPLINARITY IN COMPLEX SYSTEMS}},
Series = {{Schriftenreihe Informatik}},
Year = {{2011}},
Volume = {{36}},
Pages = {{273-279}},
Note = {{19th Interdisciplinarity Information Management Talks, Jindrichuv
   Hradec, CZECH REPUBLIC, SEP 07-09, 2011}},
Organization = {{Bundesministerium Wissenschaft \& Forschung; Grant Agcy Czech Republ;
   Univ Econ Prague; Johannes Kepler Univ Linz, Netzwerk Forschung, Lehre
   \& Praxis; KIRAS Sicherheitsforschung}},
Abstract = {{Creating portfolio has been a natural activity for years. Digital world
   due to its scope and services brings a new dimension to creating and
   sharing portfolios. In this paper the function, composition and rules
   for creating digital portfolio are described. Further we introduce basic
   trends of Web 2.0, which has the major impact on existence of multimedia
   portfolio on the internet and its rapid development. We also mention
   VSB-TU Ostrava development projects that are focused on building digital
   portfolios and users training for their individual creation and
   administration.}},
ISBN = {{978-3-85499-873-0}},
Unique-ID = {{ISI:000299185000028}},
}

@article{ ISI:000461468700002,
Author = {Unal, Yurdagul and Chowdhury, Gobinda and Kurbanoglu, Serap and
   Boustany, Joumana and Walton, Geoff},
Title = {{Research data management and data sharing behaviour of university
   researchers}},
Journal = {{INFORMATION RESEARCH-AN INTERNATIONAL ELECTRONIC JOURNAL}},
Year = {{2019}},
Volume = {{24}},
Number = {{1}},
Month = {{MAR}},
Note = {{ISIC Information Behaviour Conference, Krakow, POLAND, OCT 09-11, 2018}},
Organization = {{ISIC}},
Abstract = {{Introduction. The aim of this study is to understand how university
   researchers behave in the context of using and sharing research data in
   OA mode.
   Method. An online questionnaire survey was conducted amongst academics
   and researchers in three countries - UK, France and Turkey. There were
   26 questions to collect data on: researcher information, e.g.
   discipline, gender and experience; data sharing practices, concerns;
   familiarity with data management practices; and policies/challenges
   including knowledge of metadata and training.
   Analysis. SPSS was used to analyse the dataset, and Chi-Square tests, at
   0.05 significance level, were conducted to find out association between
   researchers' behaviour in data sharing and different areas of research
   data management (RDM).
   Findings. Findings show that OA is still not common amongst researchers.
   Data ethics and legal issues appear to be the most significant concerns
   for researchers. Most researchers have not received any training in RDM
   such as data management planning metadata, or file naming. However, most
   researchers would welcome formal training in different aspects of RDM.
   Conclusion. This study indicates directions for further research to
   understand the disciplinary differences in researchers' data access and
   management behaviour so that appropriate training and advocacy
   programmes can be developed to promote OA to research data.}},
Article-Number = {{isic1818}},
ISSN = {{1368-1613}},
ResearcherID-Numbers = {{Kurbanoglu, Serap/AAA-2236-2020
   Boustany, Joumana/I-5537-2017}},
ORCID-Numbers = {{Boustany, Joumana/0000-0001-7015-4089}},
Unique-ID = {{ISI:000461468700002}},
}

@inproceedings{ ISI:000385280000019,
Author = {Soares, Eduardo and Brandao, Pedro and Prior, Rui},
Editor = {{ElOualkadi, A and Choubani, F and ElMoussati, A}},
Title = {{Scalable Framework for Live Data Sharing Through 802.11}},
Booktitle = {{PROCEEDINGS OF THE MEDITERRANEAN CONFERENCE ON INFORMATION \&
   COMMUNICATION TECHNOLOGIES 2015 (MEDCT 2015), VOL 2}},
Series = {{Lecture Notes in Electrical Engineering}},
Year = {{2016}},
Volume = {{381}},
Pages = {{183-192}},
Note = {{Mediterranean Conference on Information and Communication Technologies
   (MedCT), Saidia, MOROCCO, MAY 07-09, 2015}},
Abstract = {{We propose a multi-platform framework for easy development of
   applications that share live or recorded data of any type in a
   classroom. It is especially aimed at training in the medical area, where
   it can make the learning process much more interactive and enriching,
   but is equally well suited for use in any type of workshop, tutorial, or
   other learning environment. The framework is browser-based, for better
   portability. In order to scale well to a large audience, the framework
   uses multicast for communication. It provides configurable reliability
   that is adaptable to data flows with different requirements, real time
   (RT) or not. It also provides security, privacy and access control
   features that are necessary in medical training environments. Finally,
   it allows session discovery and management, and multi-sender support.}},
DOI = {{10.1007/978-3-319-30298-0\_19}},
ISSN = {{1876-1100}},
ISBN = {{978-3-319-30298-0; 978-3-319-30296-6}},
ResearcherID-Numbers = {{Brandao, Pedro/G-3784-2011
   Prior, Rui/AAI-2451-2019
   }},
ORCID-Numbers = {{Brandao, Pedro/0000-0002-4681-7652
   Prior, Rui/0000-0002-3859-708X
   Soares, Eduardo/0000-0002-2694-9527}},
Unique-ID = {{ISI:000385280000019}},
}

@inproceedings{ ISI:000458717000029,
Author = {Liao, Shih-Wei and Chang, Edward Y. and Liu, Chun-Ting and Lin, Wei-Chen
   and Liao, Pin-Wei and Fu, Wei-Kang and Mei, Chung-Huan and Chang, Emily
   J.},
Book-Group-Author = {{IEEE}},
Title = {{DeepLinQ: Distributed Multi-Layer Ledgers for Privacy-Preserving Data
   Sharing}},
Booktitle = {{2018 IEEE INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND
   VIRTUAL REALITY (AIVR)}},
Year = {{2018}},
Pages = {{173-178}},
Note = {{1st IEEE International Conference on Artificial Intelligence and Virtual
   Reality (AIVR), Taichung, TAIWAN, DEC 10-12, 2018}},
Organization = {{IEEE; IEEE Comp Soc}},
Abstract = {{This paper presents requirements to DeepLinQ and its architecture.
   DeepLinQ proposes a multi-layer blockchain architecture to improve
   flexibility, accountability, and scalability through on-demand queries,
   proxy appointment, subgroup signatures, granular access control, and
   smart contracts in order to support privacy-preserving distributed data
   sharing. In this data-driven AI era where big data is the prerequisite
   for training an effective deep learning model, DeepLinQ provides a
   trusted infrastructure to enable training data collection in a
   privacy-preserved way. This paper uses healthcare data sharing as an
   application example to illustrate the key properties and design of
   DeepLinQ.}},
DOI = {{10.1109/AIVR.2018.00037}},
ISBN = {{978-1-5386-9269-1}},
Unique-ID = {{ISI:000458717000029}},
}

@article{ ISI:000473375000004,
Author = {Chawinga, Winner Dominic and Zinn, Sandy},
Title = {{Global perspectives of research data sharing: A systematic literature
   review}},
Journal = {{LIBRARY \& INFORMATION SCIENCE RESEARCH}},
Year = {{2019}},
Volume = {{41}},
Number = {{2}},
Pages = {{109-122}},
Month = {{APR}},
Abstract = {{Studies investigating data sharing from a world perspective are
   seemingly rare. By employing a quantitative design,this systematic
   review investigates and presents a comprehensive account of factors
   hampering data sharing at three levels of the global research hierarchy
   (individual, institutional and international). The study analyses
   secondary data extracted from 105 publications (n=105). Journal
   publishers and research grant organisations are key players in promoting
   data sharing activities by formulating, adopting and implementing
   policies on data sharing. Despite concerted efforts to promote data
   sharing, various factors frustrate these initiatives; they include lack
   of time and data misappropriation (individual level); data sharing
   training, absence of compensation and unfavourable internal policies
   (institutional level); and weak policies, ethical and legal norms, lack
   of data infrastructure and interoperability issues (international
   level). To counter these challenges, there is a need for research
   stakeholders to recognise researchers who share data through data
   citations, acknowledgement and incentives; invest in infrastructure,
   conduct training and advocacy programs; formulate stringent and fair
   policies. Data sharing will only become a success if research
   stakeholders apply equal efforts in managing data to that of research
   publications in general. The study offers a unique and comprehensive
   account of factors hampering data sharing from a global perspective.
   Solutions suggested could be adopted by research stakeholders in their
   efforts to enhance data sharing activities at various research levels.}},
DOI = {{10.1016/j.lisr.2019.04.004}},
ISSN = {{0740-8188}},
EISSN = {{1873-1848}},
ResearcherID-Numbers = {{Chawinga, Winner/AAZ-9775-2020
   }},
ORCID-Numbers = {{Chawinga, Winner/0000-0002-3865-4777
   Zinn, Sandy/0000-0002-0212-0036}},
Unique-ID = {{ISI:000473375000004}},
}

@inproceedings{ ISI:000383251002032,
Author = {Gellerman, Helena and Svanberg, Erik and Barnard, Yvonne},
Editor = {{Rafalski, L and Zofka, A}},
Title = {{Data sharing of transport research data}},
Booktitle = {{TRANSPORT RESEARCH ARENA TRA2016}},
Series = {{Transportation Research Procedia}},
Year = {{2016}},
Volume = {{14}},
Pages = {{2227-2236}},
Note = {{6th Transport Research Arena (TRA), Warsaw, POLAND, APR 18-21, 2016}},
Organization = {{Minist Infrastructure \& Construct Poland; Road \& Bridge Res Inst}},
Abstract = {{With the rapid progress of the development of intelligent transport
   systems over the last 15 years, the need for testing them in the real
   world and collecting data about their impact became more and more
   important. We have seen a fast growth in the number of Field Operational
   Tests (FOT) and Naturalistic Driving Studies (NDS) performed worldwide.
   The need to better understand the benefits of safety systems and the
   factors behind the occurrence of incidents and accidents have been a
   main driving force and the data has therefore been collected through
   naturalistic driving by volunteer drivers. As the number of different
   datasets has increased and so also the awareness of the substantial
   effort and funding needed to run these FOT/NDS, the interest in data
   sharing has increased worldwide.
   The availability of a common Data Sharing Framework (DSF) could highly
   facilitate a larger use of the collected FOT/NDS data. The FOT-Net Data
   project has developed such a framework, in collaboration with a variety
   of stakeholders from Europe, the US, Japan and other countries. The
   seven topics addressed by the DSF are (1) project agreements, (2) data
   and metadata descriptions, (3) data protection, (4) training, (5)
   support and research services, (6) financial models and (7) applications
   procedures. Many of the topics are general and can be used for other
   types of transport research data as well.
   There remain challenges to make data sharing possible on a global scale.
   Some of these are: the project funding schemes, leading to multiple
   schemas of ownerships of data, and the legal settings in different
   countries. On a technical level, the documentation of datasets and of
   the metadata describing the test is not always sufficient. Furthermore,
   new projects need to be made aware of the importance of inserting the
   pre-requisites for data sharing into the different project agreements
   right from the start.
   This paper describes the content of the DSF with its hands-on
   recommendations on how to prepare for and perform data sharing of
   transport research data. It also presents the status of a use case,
   implementing the DSF into the European project UDRIVE. (C) 2016
   Published by Elsevier B.V. This is an open access article under the CC
   BY-NC-ND license.}},
DOI = {{10.1016/j.trpro.2016.05.238}},
ISSN = {{2352-1465}},
Unique-ID = {{ISI:000383251002032}},
}

@article{ ISI:000465340800006,
Author = {Beardsley, Marc and Santos, Patricia and Hernandez-Leo, Davinia and
   Michos, Konstantinos},
Title = {{Ethics in educational technology research: Informing participants on
   data sharing risks}},
Journal = {{BRITISH JOURNAL OF EDUCATIONAL TECHNOLOGY}},
Year = {{2019}},
Volume = {{50}},
Number = {{3}},
Pages = {{1019-1034}},
Month = {{MAY}},
Abstract = {{Participants in educational technology research regularly share personal
   data which carries with it risks. Informing participants of these data
   sharing risks is often only done so through text contained within a
   consent form. However, conceptualizations of data sharing risks and
   knowledge of responsible data management practices among teachers and
   learners may be impoverishedlimiting the effectiveness of a consent form
   in communicating such risks in a manner that adequately supports
   participants in making informed decisions about sharing their data. At
   two high schools participating in an educational research project
   involving the use of technology in the classroom, we investigate teacher
   and student conceptions of data sharing risks and knowledge of
   responsible data management practices; and introduce a communication
   approach that attempts to better inform educational technology research
   participants of such risks. Results of this study suggest that most
   teachers have not received formal training related to responsibly
   managing data; and both teachers and students see the need for such
   training as they come to realize that their understanding of responsible
   data management is underdeveloped. Thus, efforts beyond solely
   explaining data sharing risks in an informed consent form may be needed
   in educational technology research to facilitate ethical
   self-determination.}},
DOI = {{10.1111/bjet.12781}},
ISSN = {{0007-1013}},
EISSN = {{1467-8535}},
ResearcherID-Numbers = {{Hernandez-Leo, Davinia/C-2929-2011}},
ORCID-Numbers = {{Beardsley, Marc/0000-0002-3874-0739
   Santos, Patricia/0000-0002-7337-2388
   Michos, Konstantinos/0000-0002-0573-1924
   Hernandez-Leo, Davinia/0000-0003-0548-7455}},
Unique-ID = {{ISI:000465340800006}},
}

@inproceedings{ ISI:000390305000048,
Author = {Chourasia, Amit and Wong, Mona and Mishin, Dmitry and Nadeau, David R.
   and Norman, Michael},
Book-Group-Author = {{ACM}},
Title = {{SeedMe: A scientific data sharing and collaboration platform}},
Booktitle = {{PROCEEDINGS OF XSEDE16: DIVERSITY, BIG DATA, AND SCIENCE AT SCALE}},
Year = {{2016}},
Note = {{Conference on Diversity, Big Data, and Science at Scale (XSEDE), Miami,
   FL, JUL 17-21, 2016}},
Organization = {{Intel; Dell; Hewlett Packard Enterprise; Cray; DataDirect Networks; Aeon
   Comp; Coalit Acad Sci Computat; Convergent Sci; Internet2; NVIDIA;
   OmniBond; San Diego Supercomputer Ctr; Adapt Comp; Allinea; D Wave; Gen
   Atom; Georgia State Univ; Indiana Univ, Pervas Technol Inst; iRODS;
   Lenovo; Assoc Comp Machinery}},
Abstract = {{Rapid secure data sharing and private online discussion are requirements
   for coordinating today's distributed science teams using High
   Performance Computing (HPC), visualization, and complex workflows.
   Modern HPC infrastructures enable fast computation, but the data
   produced remains within a site's storage and network environment tuned
   for performance rather than broad easy access. To share data and
   visualizations among distributed collaborators, manual efforts are
   required to move data out of HPC environments, stage it locally, bundle
   it with metadata and descriptions, manage versions, encode videos from
   visualization animations, and finally post it all online somewhere for
   secure access and discussion among project colleagues. While some of
   these tasks can be scripted, the effort remains cumbersome,
   time-consuming, and error prone. Thus, a more streamlined approach with
   a persistent infrastructure is needed.
   In this paper we describe SeedMe - the Stream Encode Explore and
   Disseminate My Experiments platform for web-based scientific data
   sharing and discussion. SeedMe provides streamlined data movement from
   HPC and desktop environments, metadata management, data descriptions,
   video encoding, secure data sharing, threaded discussion, and,
   optionally, public access for education, outreach, and training.}},
DOI = {{10.1145/2949550.2949590}},
ISBN = {{978-1-4503-4755-6}},
Unique-ID = {{ISI:000390305000048}},
}

@article{ ISI:000458163600003,
Author = {Sumitomo, Takahiro and Koshizuka, Noboru},
Title = {{Progress and Initiatives for Open Data Policy in Japan}},
Journal = {{COMPUTER}},
Year = {{2018}},
Volume = {{51}},
Number = {{12}},
Pages = {{14-23}},
Month = {{DEC}},
Abstract = {{This article examines the progress of open data policies and initiatives
   by government bodies, academia, business operators, and local public
   entities in Japan. Training at the Open Data Center of The University of
   Tokyo is stimulating wider use of open data applications.}},
DOI = {{10.1109/MC.2018.2879993}},
ISSN = {{0018-9162}},
EISSN = {{1558-0814}},
ORCID-Numbers = {{SUMITOMO, TAKAHIRO/0000-0003-1887-6820}},
Unique-ID = {{ISI:000458163600003}},
}

@article{ ISI:000455007300004,
Author = {Tenopir, C. and Christian, L. and Allard, S. and Borycz, J.},
Title = {{Research Data Sharing: Practices and Attitudes of Geophysicists}},
Journal = {{EARTH AND SPACE SCIENCE}},
Year = {{2018}},
Volume = {{5}},
Number = {{12}},
Pages = {{891-902}},
Month = {{DEC}},
Abstract = {{Open data policies have been introduced by governments, funders, and
   publishers over the past decade. Previous research showed a growing
   recognition by scientists of the benefits of data-sharing and reuse, but
   actual practices lag and are not always compliant with new regulations.
   The goal of this study is to investigate motives, attitudes, and data
   practices of the community of Earth and planetary geophysicists, a
   discipline believed to have accepting attitudes toward data sharing and
   reuse. A better understanding of the attitudes and current data-sharing
   practices of this scientific community could enable funders, publishers,
   data managers, and librarians to design systems and services that help
   scientists understand and adhere to mandates and to create practices,
   tools, and services that are scientist-focused. An online survey was
   distributed to the members of the American Geophysical Union, producing
   1,372 responses from 116 countries. The attitudes of researchers to data
   sharing and reuse were generally positive, but in practice, scientists
   had concerns about sharing their own research data. These concerns
   include the possibility of potential data misuse and the need for
   assurance of proper citation and acknowledgement. Training and
   assistance in good data management practices are lacking in many
   scientific fields and might help to alleviate these doubts.}},
DOI = {{10.1029/2018EA000461}},
ISSN = {{2333-5084}},
ResearcherID-Numbers = {{Borycz, Joshua/N-2010-2019
   }},
ORCID-Numbers = {{Borycz, Joshua/0000-0002-1505-148X
   Allard, Suzie/0000-0001-9421-3848
   Tenopir, Carol/0000-0002-9056-8251}},
Unique-ID = {{ISI:000455007300004}},
}

@article{ ISI:000464756600006,
Author = {Miao, Zelang and Xiao, Yuelong and Shi, Wenzhong and He, Yueguang and
   Gamba, Paolo and Li, Zhongbin and Samat, Alim and Wu, Lixin and Li, Jia
   and Wu, Hao},
Title = {{Integration of Satellite Images and Open Data for Impervious Surface
   Classification}},
Journal = {{IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE
   SENSING}},
Year = {{2019}},
Volume = {{12}},
Number = {{4}},
Pages = {{1120-1133}},
Month = {{APR}},
Abstract = {{Supervised learning is vital to classify impervious surface from
   satellite images. Despite its effectiveness, the training samples need
   to be provided manually, which is time consuming and labor intensive, or
   even impractical when classifying satellite images at the
   regional/global scale. This study, therefore, sets out to automatically
   generate training samples from open data, based on the fact that cities
   and urban areas are nowadays full of individual geo-referenced data,
   such as social network data. The proposed method consists of automatic
   generation of training samples based on a filtering process of open
   data, satellite image preprocessing, and impervious surface detection
   using one class classification (OCC). Two Landsat-8 Operational Land
   Imager images were selected to test the proposed method. The results
   show that the proposed method is effective in impervious surface with
   good classification accuracy. The findings in this study shine new light
   on the applications of open data in remote sensing.}},
DOI = {{10.1109/JSTARS.2019.2903585}},
ISSN = {{1939-1404}},
EISSN = {{2151-1535}},
ResearcherID-Numbers = {{Samat, Alim/H-5309-2019
   Wu, Hao/AAJ-7351-2020
   Gamba, Paolo/G-1959-2010}},
ORCID-Numbers = {{Samat, Alim/0000-0002-9091-6033
   Wu, Hao/0000-0001-5751-7885
   SHI, Wenzhong/0000-0002-3886-7027
   Gamba, Paolo/0000-0002-9576-6337}},
Unique-ID = {{ISI:000464756600006}},
}

@article{ ISI:000287205700054,
Author = {Lang, Trudie},
Title = {{Advancing Global Health Research Through Digital Technology and Sharing
   Data}},
Journal = {{SCIENCE}},
Year = {{2011}},
Volume = {{331}},
Number = {{6018}},
Pages = {{714-717}},
Month = {{FEB 11}},
Abstract = {{The imperative for improving health in the world's poorest regions lies
   in research, yet there is no question that low participation, a lack of
   trained staff, and limited opportunities for data sharing in developing
   countries impede advances in medical practice and public health
   knowledge. Extensive studies are essential to develop new treatments and
   to identify better ways to manage healthcare issues. Recent rapid
   advances in availability and uptake of digital technologies, especially
   of mobile networks, have the potential to overcome several barriers to
   collaborative research in remote places with limited access to
   resources. Many research groups are already taking advantage of these
   technologies for data sharing and capture, and these initiatives
   indicate that increasing acceptance and use of digital technology could
   promote rapid improvements in global medical science.}},
DOI = {{10.1126/science.1199349}},
ISSN = {{0036-8075}},
EISSN = {{1095-9203}},
Unique-ID = {{ISI:000287205700054}},
}

@article{ ISI:000557459300034,
Author = {Rankin, Debbie and Black, Michaela and Bond, Raymond and Wallace,
   Jonathan and Mulvenna, Maurice and Epelde, Gorka},
Title = {{Reliability of Supervised Machine Learning Using Synthetic Data in
   Health Care: Model to Preserve Privacy for Data Sharing}},
Journal = {{JMIR MEDICAL INFORMATICS}},
Year = {{2020}},
Volume = {{8}},
Number = {{7}},
Month = {{JUL}},
Abstract = {{Background: The exploitation of synthetic data in health care is at an
   early stage. Synthetic data could unlock the potential within health
   care datasets that are too sensitive for release. Several synthetic data
   generators have been developed to date; however, studies evaluating
   their efficacy and generalizability are scarce.
   Objective: This work sets out to understand the difference in
   performance of supervised machine learning models trained on synthetic
   data compared with those trained on real data.
   Methods: A total of 19 open health datasets were selected for
   experimental work. Synthetic data were generated using three synthetic
   data generators that apply classification and regression trees,
   parametric, and Bayesian network approaches. Real and synthetic data
   were used (separately) to train five supervised machine learning models:
   stochastic gradient descent, decision tree, k-nearest neighbors, random
   forest, and support vector machine. Models were tested only on real data
   to determine whether a model developed by training on synthetic data can
   used to accurately classify new, real examples. The impact of
   statistical disclosure control on model performance was also assessed.
   Results: A total of 92\% of models trained on synthetic data have lower
   accuracy than those trained on real data. Tree-based models trained on
   synthetic data have deviations in accuracy from models trained on real
   data of 0.177 (18\%) to 0.193 (19\%), while other models have lower
   deviations of 0.058 (6\%) to 0.072 (7\%). The winning classifier when
   trained and tested on real data versus models trained on synthetic data
   and tested on real data is the same in 26\% (5/19) of cases for
   classification and regression tree and parametric synthetic data and in
   21\% (4/19) of cases for Bayesian network-generated synthetic data.
   Tree-based models perform best with real data and are the winning
   classifier in 95\% (18/19) of cases. This is not the case for models
   trained on synthetic data. When tree-based models are not considered,
   the winning classifier for real and synthetic data is matched in 74\%
   (14/19), 53\% (10/19), and 68\% (13/19) of cases for classification and
   regression tree, parametric, and Bayesian network synthetic data,
   respectively. Statistical disclosure control methods did not have a
   notable impact on data utility.
   Conclusions: The results of this study are promising with small
   decreases in accuracy observed in models trained with synthetic data
   compared with models trained with real data, where both are tested on
   real data. Such deviations are expected and manageable. Tree-based
   classifiers have some sensitivity to synthetic data, and the underlying
   cause requires further investigation. This study highlights the
   potential of synthetic data and the need for further evaluation of their
   robustness. Synthetic data must ensure individual privacy and data
   utility are preserved in order to instill confidence in health care
   departments when using such data to inform policy decision-making.}},
DOI = {{10.2196/18910}},
Article-Number = {{e18910}},
EISSN = {{2291-9694}},
ORCID-Numbers = {{Bond, Raymond/0000-0002-1078-2232
   Black, Michaela/0000-0002-5140-2566
   Mulvenna, Maurice/0000-0002-1554-0785
   Wallace, Jonathan/0000-0002-8415-4001
   Epelde, Gorka/0000-0002-5179-415X}},
Unique-ID = {{ISI:000557459300034}},
}

@inproceedings{ ISI:000511427500129,
Author = {Tambouris, Efthimios and Hermans, Paul and Tarrant, David and Zotou,
   Maria and Tarabanis, Konstantinos},
Editor = {{Zuiderwijk, A and Hinnant, CC}},
Book-Group-Author = {{ACM}},
Title = {{Using Problem-based Learning and Learning Analytics in Open Data
   Education}},
Booktitle = {{PROCEEDINGS OF THE 19TH ANNUAL INTERNATIONAL CONFERENCE ON DIGITAL
   GOVERNMENT RESEARCH (DGO 2018): GOVERNANCE IN THE DATA AGE}},
Year = {{2018}},
Pages = {{863-864}},
Note = {{19th Annual International Conference on Digital Government Research
   (Dgo) - Governance in the Data Age, Delft Univ Technol, Delft,
   NETHERLANDS, MAY 30-JUN 01, 2018}},
Organization = {{Digital Govt Soc; Emerald Publishing; IOS Press; 4TU, Ctr Res Data}},
Abstract = {{Open Data initiatives worldwide are boosting with an aim to increase
   transparency and contribute to economic growth. With a global annual
   economic potential value estimated to \$3 trillion, this boost seems
   justified. Current progress however is not satisfactory. We believe a
   main reason is the lack of relevant skills and competencies. Current
   education and training activities are scarce and do not exploit
   practice-oriented learning methods such as Problem Based Learning (PBL)
   or novel assessment opportunities such as Learning Analytics (LA). As a
   result, stakeholders are missing skills related to publishing and
   reusing Open Data. The aim of this tutorial is to introduce to the
   audience Open Data education and training based on the PBL strategy and
   by exploiting LA for performance assessment. We will present a
   curriculum structure and learning content on Open Data for academia,
   business and the public sector that can be reused by all interested
   stakeholders. We will also demonstrate a university and a VET course
   model that have been designed in order to facilitate innovative and
   data-driven Open Data education and training. Finally, we will share
   lessons learnt from the practical application of the above in
   universities and businesses across multiple European countries.}},
DOI = {{10.1145/3209281.3209342}},
ISBN = {{978-1-4503-6526-0}},
ResearcherID-Numbers = {{Zotou, Maria/AAC-2692-2019
   }},
ORCID-Numbers = {{Tambouris, Efthimios/0000-0001-8036-9788}},
Unique-ID = {{ISI:000511427500129}},
}

@article{ ISI:000475944400004,
Author = {Beaulieu-Jones, Brett K. and Wu, Zhiwei Steven and Williams, Chris and
   Lee, Ran and Bhavnani, Sanjeev P. and Byrd, James Brian and Greene,
   Casey S.},
Title = {{Privacy-Preserving Generative Deep Neural Networks Support Clinical Data
   Sharing}},
Journal = {{CIRCULATION-CARDIOVASCULAR QUALITY AND OUTCOMES}},
Year = {{2019}},
Volume = {{12}},
Number = {{7}},
Month = {{JUL}},
Abstract = {{Background: Data sharing accelerates scientific progress but sharing
   individual-level data while preserving patient privacy presents a
   barrier. Methods and Results: Using pairs of deep neural networks, we
   generated simulated, synthetic participants that closely resemble
   participants of the SPRINT trial (Systolic Blood Pressure Trial). We
   showed that such paired networks can be trained with differential
   privacy, a formal privacy framework that limits the likelihood that
   queries of the synthetic participants' data could identify a real a
   participant in the trial. Machine learning predictors built on the
   synthetic population generalize to the original data set. This finding
   suggests that the synthetic data can be shared with others, enabling
   them to perform hypothesis-generating analyses as though they had the
   original trial data. Conclusions: Deep neural networks that generate
   synthetic participants facilitate secondary analyses and reproducible
   investigation of clinical data sets by enhancing data sharing while
   preserving participant privacy.}},
DOI = {{10.1161/CIRCOUTCOMES.118.005122}},
Article-Number = {{e005122}},
ISSN = {{1941-7705}},
EISSN = {{1941-7713}},
ResearcherID-Numbers = {{Byrd, James Brian/H-6674-2019
   Greene, Casey/L-2057-2015}},
ORCID-Numbers = {{Byrd, James Brian/0000-0002-0509-3520
   Greene, Casey/0000-0001-8713-9213}},
Unique-ID = {{ISI:000475944400004}},
}

@inproceedings{ ISI:000360100400019,
Author = {Ristoski, Petar and Mencia, Eneldo Loza and Paulheim, Heiko},
Editor = {{Presutti, V and Stankovic, M and Cambria, E and Cantador, I and DiIorio, A and DiNoia, T and Lange, C and Recupero, DR and Tordai, A}},
Title = {{A Hybrid Multi-strategy Recommender System Using Linked Open Data}},
Booktitle = {{SEMANTIC WEB EVALUATION CHALLENGE}},
Series = {{Communications in Computer and Information Science}},
Year = {{2014}},
Volume = {{475}},
Pages = {{150-156}},
Note = {{1st Conference on Semantic Web Evaluation Challenge (SemWebEval) as Part
   of ESWC, Crete, GREECE, MAY 25-29, 2014}},
Organization = {{PlanetData; FORGE; ANNOMARKET; Linked Data Benchmark Council; LinkedUp;
   LinkedTV; PRELIDA; MediaMixer; Fluid Operat; Videolecture net; YAHOO
   Labs; XLIKE LIME}},
Abstract = {{In this paper, we discuss the development of a hybrid multi-strategy
   book recommendation system using Linked Open Data. Our approach builds
   on training individual base recommenders and using global popularity
   scores as generic recommenders. The results of the individual
   recommenders are combined using stacking regression and rank
   aggregation. We show that this approach delivers very good results in
   different recommendation settings and also allows for incorporating
   diversity of recommendations.}},
DOI = {{10.1007/978-3-319-12024-9\_19}},
ISSN = {{1865-0929}},
EISSN = {{1865-0937}},
ISBN = {{978-3-319-12023-2}},
ORCID-Numbers = {{Paulheim, Heiko/0000-0003-4386-8195}},
Unique-ID = {{ISI:000360100400019}},
}

@inproceedings{ ISI:000630251700005,
Author = {Rajotte, Jean-Francois and Ng, Raymond T.},
Book-Group-Author = {{IEEE}},
Title = {{Private data sharing between decentralized users through the privGAN
   architecture}},
Booktitle = {{2020 IEEE 24TH INTERNATIONAL ENTERPRISE DISTRIBUTED OBJECT COMPUTING
   WORKSHOP (EDOCW 2020)}},
Series = {{IEEE International Enterprise Distributed Object Computing Conference
   Workshops-EDOCW}},
Year = {{2020}},
Pages = {{37-42}},
Note = {{24th IEEE International Enterprise Distributed Object Computing
   Conference (IEEE EDOC), ELECTR NETWORK, OCT 05-08, 2020}},
Organization = {{IEEE; IEEE Comp Soc}},
Abstract = {{More data is almost always beneficial for analysis and machine learning
   tasks. In many realistic situations however, an enterprise cannot share
   its data, either to keep a competitive advantage or to protect the
   privacy of the data sources, the enterprise's clients for example. We
   propose a method for data owners to share synthetic or fake versions of
   their data without sharing the actual data, nor the parameters of models
   that have direct access to the data. The method proposed is based on the
   privGAN architecture where local GANs are trained on their respective
   data subsets with an extra penalty from a central discriminator aiming
   to discriminate the origin of a given fake sample. We demonstrate that
   this approach, when applied to subsets of various sizes, leads to better
   utility for the owners than the utility from their real small datasets.
   The only shared pieces of information are the parameter updates of the
   central discriminator. The privacy is demonstrated with white-box
   attacks on the most vulnerable elments of the architecture and the
   results are close to random guessing. This method would apply naturally
   in a federated learning setting.}},
DOI = {{10.1109/EDOCW49879.2020.00018}},
ISSN = {{2325-6583}},
ISBN = {{978-1-7281-6471-7}},
Unique-ID = {{ISI:000630251700005}},
}

@inproceedings{ ISI:000369663000005,
Author = {Gao, Jie and Mazumdar, Suvodeep},
Editor = {{Gandon, F and Cabrio, E and Stankovic, M and Zimmermann, A}},
Title = {{Exploiting Linked Open Data to Uncover Entity Types}},
Booktitle = {{SEMANTIC WEB EVALUATION CHALLENGES}},
Series = {{Communications in Computer and Information Science}},
Year = {{2015}},
Volume = {{548}},
Pages = {{51-62}},
Note = {{2nd Conference on Semantic Web Evaluation Challenge (SemWebEval
   Challenge), Portoroz, SLOVENIA, MAY 31-JUN 04, 2015}},
Organization = {{Inria; Ontotext; Byte; STI Innsbruck; XLiMe}},
Abstract = {{Extracting structured information from text plays a crucial role in
   automatic knowledge acquisition and is at the core of any knowledge
   representation and reasoning system. Traditional methods rely on
   hand-crafted rules and are restricted by the performance of various
   linguistic pre-processing tools. More recent approaches rely on
   supervised learning of relations trained on labelled examples, which can
   be manually created or sometimes automatically generated (referred as
   distant supervision). We propose a supervised method for entity typing
   and alignment. We argue that a rich feature space can improve extraction
   accuracy and we propose to exploit Linked Open Data (LOD) for feature
   enrichment. Our approach is tested on task-2 of the Open Knowledge
   Extraction challenge, including automatic entity typing and alignment.
   Our approach demonstrate that by combining evidences derived from LOD
   (e.g. DBpedia) and conventional lexical resources (e.g. WordNet) (i)
   improves the accuracy of the supervised induction method and (ii)
   enables easy matching with the Dolce+DnS Ultra Lite ontology classes.}},
DOI = {{10.1007/978-3-319-25518-7\_5}},
ISSN = {{1865-0929}},
EISSN = {{1865-0937}},
ISBN = {{978-3-319-25518-7; 978-3-319-25517-0}},
ResearcherID-Numbers = {{GAO, JIE/AAB-4270-2020
   }},
ORCID-Numbers = {{GAO, JIE/0000-0002-3610-8748
   Mazumdar, Suvodeep/0000-0002-0748-7638}},
Unique-ID = {{ISI:000369663000005}},
}

@article{ ISI:000451775000001,
Author = {Tang, Chunlei and Plasek, Joseph M. and Bates, David W.},
Title = {{Rethinking Data Sharing at the Dawn of a Health Data Economy: A
   Viewpoint}},
Journal = {{JOURNAL OF MEDICAL INTERNET RESEARCH}},
Year = {{2018}},
Volume = {{20}},
Number = {{11}},
Month = {{NOV 22}},
Abstract = {{A health data economy has begun to form, but its rise has been tempered
   by the profound lack of sharing of both data and data products such as
   models, intermediate results, and annotated training corpora, and this
   severely limits the potential for triggering economic cluster effects.
   Economic cluster effects represent a means to elicit benefit from
   economies of scale from internal data innovations and are beneficial
   because they may mitigate challenges from external sources. Within
   institutions, data product sharing is needed to spark data
   entrepreneurship and data innovation, and cross-institutional sharing is
   also critical, especially for rare conditions.}},
DOI = {{10.2196/11519}},
Article-Number = {{e11519}},
ISSN = {{1438-8871}},
ResearcherID-Numbers = {{Bates, David/AAE-7283-2019
   Tang, Chunlei/AAC-1438-2020
   Plasek, Joseph/AAI-8263-2020
   }},
ORCID-Numbers = {{Tang, Chunlei/0000-0002-6460-0246
   Plasek, Joseph/0000-0002-9686-3876
   Bates, David/0000-0001-6268-1540}},
Unique-ID = {{ISI:000451775000001}},
}

@inproceedings{ ISI:000189427900062,
Author = {Callahan, RN and Hubbard, KM and Strong, SD},
Book-Group-Author = {{ASEM}},
Title = {{Collaborative student projects and data sharing in the coalition for
   Improved Manufacturing Education}},
Booktitle = {{ENGINEERING MANAGEMENT IN THE GLOBAL ENVIRONMENTS, PROCEEDINGS}},
Year = {{2002}},
Pages = {{411-415}},
Note = {{23rd National Conference of the
   American-Society-for-Engineering-Management, Tampa, FL, OCT 02-05, 2002}},
Organization = {{Univ S Florida, Ind \& Management Syst; Amer Soc Engn Management}},
Abstract = {{Four colleges in Illinois and Missouri have joined efforts to form the
   Coalition for Improved Manufacturing Education to enhance manufacturing
   skills and opportunities throughout the Midwest. The members of the
   coalition include two and four year technology, technical management,
   and engineering colleges and universities which share a vision for
   improved manufacturing training and interaction with industry. Two
   primary mechanisms will be employed by which the institutions in the
   coalition will collaborate; cooperative student projects, often
   involving industry partners; and shared data collection and
   distribution. This paper describes the mechanics of these collaborations
   and the expected results.}},
Unique-ID = {{ISI:000189427900062}},
}

@article{ ISI:000571567200001,
Author = {Abuhammad, Sawsan and Alzoubi, Karem H. and Al-Azzam, I, Sayer and
   Karasneh, Reema A.},
Title = {{Knowledge and Practice of Patients' Data Sharing and Confidentiality
   Among Nurses in Jordan}},
Journal = {{JOURNAL OF MULTIDISCIPLINARY HEALTHCARE}},
Year = {{2020}},
Volume = {{13}},
Pages = {{935-942}},
Abstract = {{Background: The key patient rights entail respecting human decency,
   receiving healthcare services of high-quality, the right to information,
   the initial agreement of the patient to medical intervention, respecting
   privacy and personal life, and sustaining care and treatment. This study
   aims to survey the knowledge and practice of nurses in various
   healthcare industries toward sharing and confidentiality of patients'
   data.
   Methods: A descriptive cross-sectional design was employed through an
   online survey from May to June 2020. The authors sent a developed tool
   containing 19 statements reflecting the understanding of nurses'
   knowledge and practice of privacy and sharing of data required to
   safeguard patient privacy. A total of 800 nurses agreed to participate
   in the study out of 1000 nurses.
   Results: Roughly, all participants agreed that junior nurses should
   participate in a data sharing and confidentiality course before engaging
   in practice. Regarding institution policies for data sharing and
   protection, many nurses agreed that there are special recommendations
   and instructions from the institution in which they work to exchange
   patient information among nurses and the medical staff. The predictors
   of sharing practices and confidentiality among nurses include age,
   gender, marriage status, and attending a security course before
   practice. Young age, female, not attending a data sharing course, and
   single nurses are less engaging with data sharing and confidentiality of
   the patients for unauthorized patients.
   Conclusion: A significant proportion of the staff had appropriate
   practices that ensured data security. However, practices that ensure
   patient confidentiality in the aspects of access, sharing, and
   transferring of patient data need improvement. Training is essential
   since it will have a beneficial relationship with knowledge, opinions,
   views, and actions. Thus, planning continuous training on policies and
   regulations about data safety and privacy may assist in improving
   healthcare setting practices.}},
DOI = {{10.2147/JMDH.S269511}},
ISSN = {{1178-2390}},
ORCID-Numbers = {{Karasneh, Reema/0000-0002-2919-1280}},
Unique-ID = {{ISI:000571567200001}},
}

@article{ ISI:000229424100003,
Author = {Kirchhoff, K and Vergyri, D},
Title = {{Cross-dialectal data sharing for acoustic modeling in Arabic speech
   recognition}},
Journal = {{SPEECH COMMUNICATION}},
Year = {{2005}},
Volume = {{46}},
Number = {{1}},
Pages = {{37-51}},
Month = {{MAY}},
Abstract = {{Many of the world's languages have a multitude of dialects which differ
   considerably from each other in their linguistic properties. Dialects
   are often spoken rather than written varieties; the development of
   automatic speech recognition systems for dialects therefore requires the
   collection and transcription of large amounts of dialectal speech. In
   those cases where sufficient training data is not available, acoustic
   and/or language models may benefit from additional data from different
   though related dialects. In this study we investigate the feasibility of
   cross-dialectal data sharing for acoustic modeling using two different
   varieties of Arabic, Modern Standard Arabic and Egyptian Colloquial
   Arabic. An obstacle to this type of data sharing is the Arabic writing
   system, which lacks short vowels and other phonetic information. We
   address this problem by developing automatic procedures to restore the
   missing information based on morphological, contextual and acoustic
   knowledge. These procedures are evaluated with respect to the relative
   contributions of different knowledge sources and with respect to their
   effect on the overall recognition system. We demonstrate that
   cross-dialectal data sharing leads to significant reductions in word
   error rate. (c) 2005 Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.specom.2005.01.004}},
ISSN = {{0167-6393}},
EISSN = {{1872-7182}},
Unique-ID = {{ISI:000229424100003}},
}

@inproceedings{ ISI:000644436100064,
Author = {Gasco-Hernandez, M. and Martin, E. G. and Reggi, L. and Pyo, S. and
   Luna-Reyes, L. F.},
Editor = {{Hinnant, CC and Ojo, A}},
Title = {{Citizen Co-production through Open Data: Cases of Citizen Training and
   Engagement}},
Booktitle = {{DG.O 2017: THE PROCEEDINGS OF THE 18TH ANNUAL INTERNATIONAL CONFERENCE
   ON DIGITAL GOVERNMENT RESEARCH: INNOVATIONS AND TRANSFORMATIONS IN
   GOVERNMENT}},
Year = {{2017}},
Pages = {{562-565}},
Note = {{18th Annual International Conference on Digital Government Research
   (DG.O) - Innovations and Transformations in Government, City New York
   Univ, Coll Staten Island, Staten Island, NY, JUN 07-09, 2017}},
Organization = {{Digital Govt Soc; ICF; City New York Univ, Coll Staten Island, Sch
   Business; City New York Univ, iSecure Lab; IOS Press; Rutgers Univ, I
   DSLA Inst; Journal Informat; Emerald Publishing Grp}},
Abstract = {{In recent years, the role of the citizen in the delivery of government
   services is being reinterpreted, changing their role from passive
   consumers to partners or co-producer of services. This trend is
   consistent with the core values of the open data movement: Innovation,
   collaboration, and participation. In this paper, we introduce three
   cases of citizen co-production of services through open data. The three
   cases focus particularly in the evaluation and assessment of services
   and policies. Our ongoing research explores the effectiveness of these
   co-production experiences in increasing citizen participation.(1)}},
DOI = {{10.1145/3085228.3085252}},
ISBN = {{978-1-4503-5317-5}},
Unique-ID = {{ISI:000644436100064}},
}

@article{ ISI:000431154300002,
Author = {Argimon, Silvia and Abudahab, Khalil and Goater, Richard J. E. and
   Fedosejev, Artemij and Bhai, Jyothish and Glasner, Corinna and Feil,
   Edward J. and Holden, Matthew T. G. and Yeats, Corin A. and Grundmann,
   Hajo and Spratt, Brian G. and Aanensen, David M.},
Title = {{Microreact: visualizing and sharing data for genomic epidemiology and
   phylogeography}},
Journal = {{MICROBIAL GENOMICS}},
Year = {{2016}},
Volume = {{2}},
Number = {{11}},
Month = {{NOV}},
Abstract = {{Visualization is frequently used to aid our interpretation of complex
   datasets. Within microbial genomics, visualizing the relationships
   between multiple genomes as a tree provides a framework onto which
   associated data (geographical, temporal, phenotypic and epidemiological)
   are added to generate hypotheses and to explore the dynamics of the
   system under investigation. Selected static images are then used within
   publications to highlight the key findings to a wider audience. However,
   these images are a very inadequate way of exploring and interpreting the
   richness of the data. There is, therefore, a need for flexible,
   interactive software that presents the population genomic outputs and
   associated data in a user-friendly manner for a wide range of end users,
   from trained bioinformaticians to front-line epidemiologists and health
   workers. Here, we present Microreact, a web application for the easy
   visualization of datasets consisting of any combination of trees,
   geographical, temporal and associated metadata. Data files can be
   uploaded to Microreact directly via the web browser or by linking to
   their location (e.g. from Google Drive/Dropbox or via API), and an
   integrated visualization via trees, maps, timelines and tables provides
   interactive querying of the data. The visualization can be shared as a
   permanent web link among collaborators, or embedded within publications
   to enable readers to explore and download the data. Microreact can act
   as an end point for any tool or bioinformatic pipeline that ultimately
   generates a tree, and provides a simple, yet powerful, visualization
   method that will aid research and discovery and the open sharing of
   datasets.}},
DOI = {{10.1099/mgen.0.000093}},
Article-Number = {{000093}},
ISSN = {{2057-5858}},
ResearcherID-Numbers = {{Holden, Matthew/K-6449-2014
   }},
ORCID-Numbers = {{Holden, Matthew/0000-0002-4958-2166
   Bhai, Jyothish/0000-0002-2815-7478}},
Unique-ID = {{ISI:000431154300002}},
}

@article{ ISI:000554145900001,
Author = {Stewart, Christopher and Lazzarini, Michele and Luna, Adrian and Albani,
   Sergio},
Title = {{Deep Learning with Open Data for Desert Road Mapping}},
Journal = {{REMOTE SENSING}},
Year = {{2020}},
Volume = {{12}},
Number = {{14}},
Month = {{JUL}},
Abstract = {{The availability of free and open data from Earth observation programmes
   such as Copernicus, and from collaborative projects such as Open Street
   Map (OSM), enables low cost artificial intelligence (AI) based
   monitoring applications. This creates opportunities, particularly in
   developing countries with scarce economic resources, for large-scale
   monitoring in remote regions. A significant portion of Earth's surface
   comprises desert dune fields, where shifting sand affects infrastructure
   and hinders movement. A robust, cost-effective and scalable methodology
   is proposed for road detection and monitoring in regions covered by
   desert sand. The technique uses Copernicus Sentinel-1 synthetic aperture
   radar (SAR) satellite data as an input to a deep learning model based on
   the U-Net architecture for image segmentation. OSM data is used for
   model training. The method comprises two steps: The first involves
   processing time series of Sentinel-1 SAR interferometric wide swath (IW)
   acquisitions in the same geometry to produce multitemporal backscatter
   and coherence averages. These are divided into patches and matched with
   masks of OSM roads to form the training data, the quantity of which is
   increased through data augmentation. The second step includes the U-Net
   deep learning workflow. The methodology has been applied to three
   different dune fields in Africa and Asia. A performance evaluation
   through the calculation of the Jaccard similarity coefficient was
   carried out for each area, and ranges from 84\% to 89\% for the best
   available input. The rank distance, calculated from the completeness and
   correctness percentages, was also calculated and ranged from 75\% to
   80\%. Over all areas there are more missed detections than false
   positives. In some cases, this was due to mixed infrastructure in the
   same resolution cell of the input SAR data. Drift sand and dune
   migration covering infrastructure is a concern in many desert regions,
   and broken segments in the resulting road detections are sometimes due
   to sand burial. The results also show that, in most cases, the
   Sentinel-1 vertical transmit-vertical receive (VV) backscatter averages
   alone constitute the best input to the U-Net model. The detection and
   monitoring of roads in desert areas are key concerns, particularly given
   a growing population increasingly on the move.}},
DOI = {{10.3390/rs12142274}},
Article-Number = {{2274}},
EISSN = {{2072-4292}},
ORCID-Numbers = {{Stewart, Christopher/0000-0001-6965-8271}},
Unique-ID = {{ISI:000554145900001}},
}

@inproceedings{ ISI:000455712300106,
Author = {Nechaev, Yaroslav and Corcoglioniti, Francesco and Giuliano, Claudio},
Editor = {{Cuzzocrea, A and Allan, J and Paton, N and Srivastava, D and Agrawal, R and Broder, A and Zaki, M and Candan, S and Labrinidis, A and Schuster, A and Wang, H}},
Title = {{Type Prediction Combining Linked Open Data and Social Media}},
Booktitle = {{CIKM'18: PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON
   INFORMATION AND KNOWLEDGE MANAGEMENT}},
Year = {{2018}},
Pages = {{1033-1042}},
Note = {{27th ACM International Conference on Information and Knowledge
   Management (CIKM), Torino, ITALY, OCT 22-26, 2018}},
Organization = {{Assoc Comp Machinery; Assoc Comp Machinery Special Interest Grp Informat
   Retrieval; Assoc Comp Machinery SIGWEB; Univ Trieste}},
Abstract = {{Linked Open Data (LOD) and social media often contain the
   representations of the same real-world entities, such as persons and
   organizations. These representations are increasingly interlinked,
   making it possible to combine and leverage both LOD and social media
   data in prediction problems, complementing their relative strengths:
   while LOD knowledge is highly structured but also scarce and obsolete
   for some entities, social media data provide real-time updates and
   increased coverage, albeit being mostly unstructured.
   In this paper, we investigate the feasibility of using social media data
   to perform type prediction for entities in a LOD knowledge graph. We
   discuss how to gather training data for such a task, and how to build an
   efficient domain-independent vector representation of entities based on
   social media data. Our experiments on several type prediction tasks
   using DBpedia and Twitter data show the effectiveness of this
   representation, both alone and combined with knowledge graph-based
   features, suggesting its potential for ontology population.}},
DOI = {{10.1145/3269206.3271781}},
ISBN = {{978-1-4503-6014-2}},
ORCID-Numbers = {{Corcoglioniti, Francesco/0000-0002-0656-606X}},
Unique-ID = {{ISI:000455712300106}},
}

@article{ ISI:000359933300015,
Author = {Rufat, Samuel},
Title = {{Open Data, Political Crisis and Guerrilla Cartography}},
Journal = {{ACME-AN INTERNATIONAL E-JOURNAL FOR CRITICAL GEOGRAPHIES}},
Year = {{2015}},
Volume = {{14}},
Number = {{1}},
Pages = {{260-282}},
Abstract = {{Open data and the geoweb have emerged, along with the rhetoric of
   democratization and a promise that increased user participation would
   lead to more empowered citizens. Recently, European rules have attempted
   to make the availability and re-use of data from everywhere much easier.
   The EU Open Data rules are shifting issues from finding information to
   selecting the more relevant data and enabling new approaches to the
   real-time scrutiny of powerful institutions. However, geography, open
   data and the Internet are obviously not intrinsically subversive. Moving
   from transparency to accountability, and from critical thinking to
   political leverage, requires making sense of data and empowering people
   with it. This suggests that crowdsourcing geography is not so much about
   collaboratively distributing the production of data but instead about
   shifting the production of meaning from the few to the many, soliciting
   contributions for the critical analysis of data, openly distributing
   problem-solving and using the exchanges between people from different
   backgrounds all across the world to construct the interpretation.
   Crowdsourcing geography reduces information asymmetry and enables power
   strategies, deconstruction and counter-hegemonic initiatives to jump
   spatial scales, thereby allowing them to leverage public opinion on a
   global scale. This is an opportunity for guerrilla cartography,
   transforming data and geographical knowledge into real-time leverage and
   coming unexpectedly, because it can be launched from virtually any
   place, crowdsourced, and spawn followers around the world. However,
   shifting the production of meaning from the few to the many requires
   more trained brains than dot. com domains. What matters most is grasping
   `dead' data, giving it `live' meaning, producing reusable information
   just in time, rapidly transforming data into political leverage and
   sharing it in an efficient manner. This paper showcases the
   possibilities of crowdsourcing geography and guerrilla cartography by
   using the political crisis in Romania that took place during the summer
   of 2012.}},
ISSN = {{1492-9732}},
ORCID-Numbers = {{Rufat, Samuel/0000-0001-6356-1233}},
Unique-ID = {{ISI:000359933300015}},
}

@article{ ISI:000526381800052,
Author = {Lu, Yunlong and Huang, Xiaohong and Dai, Yueyue and Maharjan, Sabita and
   Zhang, Yan},
Title = {{Blockchain and Federated Learning for Privacy-Preserved Data Sharing in
   Industrial IoT}},
Journal = {{IEEE TRANSACTIONS ON INDUSTRIAL INFORMATICS}},
Year = {{2020}},
Volume = {{16}},
Number = {{6}},
Pages = {{4177-4186}},
Month = {{JUN}},
Abstract = {{The rapid increase in the volume of data generated from connected
   devices in industrial Internet of Things paradigm, opens up new
   possibilities for enhancing the quality of service for the emerging
   applications through data sharing. However, security and privacy
   concerns (e.g., data leakage) are major obstacles for data providers to
   share their data in wireless networks. The leakage of private data can
   lead to serious issues beyond financial loss for the providers. In this
   article, we first design a blockchain empowered secure data sharing
   architecture for distributed multiple parties. Then, we formulate the
   data sharing problem into a machine-learning problem by incorporating
   privacy-preserved federated learning. The privacy of data is
   well-maintained by sharing the data model instead of revealing the
   actual data. Finally, we integrate federated learning in the consensus
   process of permissioned blockchain, so that the computing work for
   consensus can also be used for federated training. Numerical results
   derived from real-world datasets show that the proposed data sharing
   scheme achieves good accuracy, high efficiency, and enhanced security.}},
DOI = {{10.1109/TII.2019.2942190}},
ISSN = {{1551-3203}},
EISSN = {{1941-0050}},
ORCID-Numbers = {{Maharjan, Sabita/0000-0002-4616-8488
   Dai, Yueyue/0000-0002-2163-987X}},
Unique-ID = {{ISI:000526381800052}},
}

@article{ ISI:000424616400007,
Author = {Ayre, Lori Bowen and Craner, Jim},
Title = {{Open Data: What It Is and Why You Should Care}},
Journal = {{PUBLIC LIBRARY QUARTERLY}},
Year = {{2017}},
Volume = {{36}},
Number = {{2}},
Pages = {{173-184}},
Abstract = {{Open data is information that is provided by public entities to be
   accessed and reused. Publishing open data is an excellent way to improve
   an organization's transparency and provide insight into the value of the
   organization. Libraries are uniquely positioned to assist their patrons
   in the use of open data by making them aware of it and helping them
   access and use it. This article provides a short history of open data
   and explores ways that library trailblazers are already using open data
   as well as contributing open data for others to use. Getting started
   with open data involves identifying key open data resources such as
   data. gov, identifying library information that would be beneficial to
   publish, and creating programs that provide digital literacy training
   and create opportunities for patrons to engage with open data in new and
   creative ways.}},
DOI = {{10.1080/01616846.2017.1313045}},
ISSN = {{0161-6846}},
EISSN = {{1541-1540}},
ORCID-Numbers = {{Ayre, Lori/0000-0002-3604-9530
   Craner, Jim/0000-0001-9347-101X}},
Unique-ID = {{ISI:000424616400007}},
}

@article{ ISI:000364024300033,
Author = {Vanderbilt, Kristin L. and Lin, Chau-Chin and Lu, Sheng-Shan and Kassim,
   Abd Rahman and He, Honglin and Guo, Xuebing and Gil, Inigo San and
   Blankman, David and Porter, John H.},
Title = {{Fostering ecological data sharing: collaborations in the International
   Long Term Ecological Research Network}},
Journal = {{ECOSPHERE}},
Year = {{2015}},
Volume = {{6}},
Number = {{10}},
Month = {{OCT}},
Abstract = {{The International Long Term Ecological Research (ILTER) Network was
   established in 1993 and is now composed of thirty-eight national
   networks representing a diversity of ecosystems around the globe. Data
   generated by the ILTER Network are valuable for scientists addressing
   broad spatial and temporal scale research questions, but only if these
   data can be easily discovered, accessed, and understood. Challenges to
   publishing ILTER data have included unequal distribution among networks
   of information management expertise, user-friendly tools, and resources.
   Language and translation have also been issues. Despite these
   significant obstacles, ILTER information managers have formed grassroots
   partnerships and collaborated to provide information management
   training, adopt a common metadata standard, develop information
   management tools useful throughout the network, and organize
   scientist/information manager workshops that encourage scientists to
   share and integrate data. Throughout this article, we share lessons
   learned from the successes of these grassroots international
   partnerships to inform others who wish to collaborate internationally on
   projects that depend on data sharing entailing similar management
   challenges.}},
DOI = {{10.1890/ES14-00281.1}},
Article-Number = {{204}},
ISSN = {{2150-8925}},
ORCID-Numbers = {{Gaiser, Evelyn/0000-0003-2065-4821
   Porter, John/0000-0003-3118-5784
   Vanderbilt, Kristin/0000-0003-1439-2204}},
Unique-ID = {{ISI:000364024300033}},
}

@article{ ISI:000541831800015,
Author = {Kaari, Jennifer},
Title = {{Researchers at Arab Universities Hold Positive Views on Research Data
   Management and Data Sharing}},
Journal = {{EVIDENCE BASED LIBRARY AND INFORMATION PRACTICE}},
Year = {{2020}},
Volume = {{15}},
Number = {{2}},
Pages = {{169-171}},
Month = {{JUN 15}},
Abstract = {{Objective - To investigate researchers' practices and attitudes
   regarding research data management and data sharing.
   Design - Email survey.
   Setting - Universities in Egypt, Jordan, and Saudi Arabia.
   Subjects - Surveys were sent to 4,086 academic faculty researchers.
   Methods - The survey was emailed to faculty at three Arab universities,
   targeting faculty in the life sciences and engineering. The survey was
   created using Google Docs and remained open for five months.
   Participants were asked basic demographic questions, questions regarding
   their research data and metadata practices, and questions regarding
   their data sharing practices.
   Main Results - The authors received 337 responses, for a response rate
   of 8\%. The results showed that 48.4\% of respondents had a data
   management plan and that 97\% were responsible for preserving their own
   data. Most respondents stored their research data on their personal
   storage devices. The authors found that 64.4\% of respondents reported
   sharing their research data. Respondents most frequently shared their
   data by publishing in a data research journal, sharing through academic
   social networks such as ResearchGate, and providing data upon request to
   peers. Only 5.1\% of respondents shared data through an open data
   repository. Of those who did not share data, data privacy and
   confidentiality were the most common reasons cited. Of the respondents
   who did share their data, contributing to scientific progress and
   increased citation and visibility were the primary reasons for doing so.
   A total of 59.6\% of respondents stated that they needed more training
   in research data management from their universities.
   Conclusion - The authors conclude that researchers at Arab universities
   are still primarily responsible for their own data and that data
   management planning is still a new concept to most researchers. For the
   most part, the researchers had a positive attitude toward data sharing,
   although depositing data in open repositories is still not a widespread
   practice. The authors conclude that in order to encourage strong data
   management practices and open data sharing among Arab university
   researchers, more training and institutional support is needed.}},
DOI = {{10.18438/eblip29746}},
ISSN = {{1715-720X}},
Unique-ID = {{ISI:000541831800015}},
}

@article{ ISI:000642570800003,
Author = {Gupta, Ananya and Watson, Simon and Yin, Hujun},
Title = {{Deep learning-based aerial image segmentation with open data for
   disaster impact assessment}},
Journal = {{NEUROCOMPUTING}},
Year = {{2021}},
Volume = {{439}},
Pages = {{22-33}},
Month = {{JUN 7}},
Abstract = {{Satellite images are an extremely valuable resource in the aftermath of
   natural disasters such as hurricanes and tsunamis where they can be used
   for risk assessment and disaster management. In order to provide timely
   and actionable information for disaster response, a framework utilising
   segmentation neural networks is proposed to identify impacted areas and
   accessible roads in post-disaster scenarios. The effectiveness of
   pretraining with ImageNet-for the task of aerial image segmentation has
   been analysed and performances of popular segmentation models compared.
   Experimental results show that pre training on ImageNet usually improves
   the segmentation performance for a number of models. Open data available
   from OpenStreetMap (OSM) is used for training, forgoing the need for
   time-consuming manual annotation. The method also makes use of graph
   theory to update road network data available from OSM and to detect the
   changes caused by a natural disaster. Extensive experiments on data from
   the 2018 tsunami that struck Palu, Indonesia show the effectiveness of
   the proposed framework. ENetSeparable, with 30\% fewer parameters
   compared to ENet, achieved comparable segmentation results to that of
   the stateof-the-art networks.
   (c) 2021 Elsevier B.V. All rights reserved.}},
DOI = {{10.1016/j.neucom.2020.02.139}},
ISSN = {{0925-2312}},
EISSN = {{1872-8286}},
Unique-ID = {{ISI:000642570800003}},
}

@inproceedings{ ISI:000440863100009,
Author = {Chowdhury, Gobinda and Boustany, Joumana and Kurbanoglu, Serap and Unal,
   Yurdagul and Walton, Geoff},
Editor = {{Choemprayong, S and Crestani, F and Cunningham, SJ}},
Title = {{Preparedness for Research Data Sharing: A Study of University
   Researchers in Three European Countries}},
Booktitle = {{DIGITAL LIBRARIES: DATA, INFORMATION, AND KNOWLEDGE FOR DIGITAL LIVES}},
Series = {{Lecture Notes in Computer Science}},
Year = {{2017}},
Volume = {{10647}},
Note = {{19th International Conference on Asia-Pacific Digital Libraries (ICADL)
   - Data, Information, and Knowledge for Digital Lives, Chulalongkorn
   Univ, Fac Arts, Dept Lib Sci, Bangkok, THAILAND, NOV 13-15, 2017}},
Abstract = {{Many government and funding bodies around the world have been advocating
   open access to research data, arguing that such open access can bring a
   significant degree of economic and social benefit. However, the question
   remains, do researchers themselves want to share their research data,
   and even if they do how far they are prepared to make this happen? In
   this paper we report on an international survey involving university
   researchers in three countries, viz. UK, France and Turkey. We found
   that researchers have a number of concerns for data sharing, and in
   general there is a lack of understanding of the requirements for making
   data publicly available and accessible. We note that significant
   training and advocacy will be required to make the vision of data
   sharing a reality.}},
DOI = {{10.1007/978-3-319-70232-2\_9}},
ISSN = {{0302-9743}},
EISSN = {{1611-3349}},
ISBN = {{978-3-319-70232-2; 978-3-319-70231-5}},
ResearcherID-Numbers = {{Boustany, Joumana/I-5537-2017
   chowdhury, gobinda/K-3518-2019}},
ORCID-Numbers = {{Boustany, Joumana/0000-0001-7015-4089
   chowdhury, gobinda/0000-0002-2122-9547}},
Unique-ID = {{ISI:000440863100009}},
}

@inproceedings{ ISI:000502074700012,
Author = {Barclay, Iain and Preece, Alun and Taylor, Ian and Verma, Dinesh},
Editor = {{Pham, T}},
Title = {{A Conceptual Architecture for Contractual Data Sharing in a
   Decentralised Environment}},
Booktitle = {{ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING FOR MULTI-DOMAIN OPERATIONS
   APPLICATIONS}},
Series = {{Proceedings of SPIE}},
Year = {{2019}},
Volume = {{11006}},
Note = {{Conference on Artificial Intelligence and Machine Learning for
   Multi-Domain Operations Applications, Baltimore, MD, APR 15-17, 2019}},
Organization = {{SPIE}},
Abstract = {{Machine Learning systems rely on data for training, input and ongoing
   feedback and validation. Data in the field can come from varied sources,
   often anonymous or unknown to the ultimate users of the data. Whenever
   data is sourced and used, its consumers need assurance that the data
   accuracy is as described, that the data has been obtained legitimately,
   and they need to understand the terms under which the data is made
   available so that they can honour them. Similarly, suppliers of data
   require assurances that their data is being used legitimately by
   authorised parties, in accordance with their terms, and that usage is
   appropriately recompensed. Furthermore, both parties may want to agree
   on a specific set of quality of service (QoS) metrics, which can be used
   to negotiate service quality based on cost, and then receive affirmation
   that data is being supplied within those agreed QoS levels. Here we
   present a conceptual architecture which enables data sharing agreements
   to be encoded and computationally enforced, remuneration to be made when
   required, and a trusted audit trail to be produced for later analysis or
   reproduction of the environment. Our architecture uses blockchain-based
   distributed ledger technology, which can facilitate transactions in
   situations where parties do not have an established trust relationship
   or centralised command and control structures. We explore techniques to
   promote faith in the accuracy of the supplied data, and to let data
   users determine trade-offs between data quality and cost. Our system is
   exemplified through consideration of a case study using multiple data
   sources from different parties to monitor traffic levels in urban
   locations.}},
DOI = {{10.1117/12.2518644}},
Article-Number = {{UNSP 110060G}},
ISSN = {{0277-786X}},
EISSN = {{1996-756X}},
ISBN = {{978-1-5106-2678-2}},
ORCID-Numbers = {{Taylor, Ian/0000-0001-5040-0772}},
Unique-ID = {{ISI:000502074700012}},
}

@article{ ISI:000360583200015,
Author = {Laney, Christine M. and Pennington, Deana D. and Tweedie, Craig E.},
Title = {{Filling the gaps: sensor network use and data-sharing practices in
   ecological research}},
Journal = {{FRONTIERS IN ECOLOGY AND THE ENVIRONMENT}},
Year = {{2015}},
Volume = {{13}},
Number = {{7}},
Pages = {{363-368}},
Month = {{SEP}},
Abstract = {{Automated sensors are ubiquitous in ecological ``research networks{''}
   and academic-led studies. We conducted a survey of academic ecologists
   to assess the extent of sensor use and how sensor-captured data are
   managed. There were 135 survey respondents, each representing a unique
   research group and collectively representing more than 1800 researchers
   from more than 90 universities in the US. This pool of represented
   academic researchers collectively matched the financial expenditure,
   sensor use, and data volumes of several large national research
   networks. Few respondents reported the use of metadata and workflows
   while almost 70\% stored data locally instead of within institutional
   archives, though most recognized the importance of doing the latter.
   Most respondents also indicated that their research could be enhanced by
   taking advantage of cyber-expertise and software tools. Access to
   sensor-generated datasets may be improved by several means, including
   the promotion of targeted training, high-profile studies showcasing
   researchers' participation in large-scale syntheses, and incentives for
   industry to develop, adopt, or adapt technologies that facilitate data
   documentation, discovery, and sharing.}},
DOI = {{10.1890/140341}},
ISSN = {{1540-9295}},
EISSN = {{1540-9309}},
ResearcherID-Numbers = {{Laney, Christine/B-7867-2010}},
ORCID-Numbers = {{Laney, Christine/0000-0002-4944-2083}},
Unique-ID = {{ISI:000360583200015}},
}

@article{ ISI:000338747300001,
Author = {Guthrie, Dawn M. and Pitman, Robyn and Fletcher, Paula C. and Hirdes,
   John P. and Stolee, Paul and Poss, Jeffrey W. and Papaioannou, Alexandra
   and Berg, Katherine and Ezekiel, Helen Janzen},
Title = {{Data sharing between home care professionals: a feasibility study using
   the RAI Home Care instrument}},
Journal = {{BMC GERIATRICS}},
Year = {{2014}},
Volume = {{14}},
Month = {{JUN 30}},
Abstract = {{Background: Across Ontario, home care professionals collect standardized
   information on each client using the Resident Assessment for Home Care
   (RAI-HC). However, this information is not consistently shared with
   those professionals who provide services in the client's home. In this
   pilot study, we examined the feasibility of sharing data, from the
   RAI-HC, between care coordinators and service providers.
   Methods: All participants were involved in a one-day training session on
   the RAI-HC. The care coordinators shared specific outputs from the
   RAI-HC, including the embedded health index scales, with their
   contracted physiotherapy and occupational therapy service providers. Two
   focus groups were held, one with care coordinators (n = 4) and one with
   contracted service providers (n = 6). They were asked for their opinions
   on the positive aspects of the project and areas for improvement.
   Results: The focus groups revealed a number of positive outcomes related
   to the project including the use of a falls prevention brochure and an
   increased level of communication between professionals. The participants
   also cited multiple areas for improvement related to data sharing (e.
   g., time constraints, data being sent in a timely fashion) and to their
   standard practices in the community (e. g., busy workloads, difficulties
   in data sharing, duplication of assessments between professionals).
   Conclusions: Home care professionals were able to share select pieces of
   information generated from the RAI-HC system and this project enhanced
   the level of communication between the two groups of professionals.
   However, a single information session was not adequate training for the
   rehabilitation professionals, who do not use the RAI-HC as part of
   normal practice. Better education, ongoing support and timely access to
   the RAI-HC data are some ways to improve the usefulness of this
   information for busy home care providers.}},
DOI = {{10.1186/1471-2318-14-81}},
Article-Number = {{81}},
EISSN = {{1471-2318}},
ResearcherID-Numbers = {{Guthrie, Dawn/AAO-6453-2021
   Stolee, Paul/ABB-2660-2020}},
ORCID-Numbers = {{Guthrie, Dawn/0000-0003-3241-6580
   Stolee, Paul/0000-0002-5685-0843}},
Unique-ID = {{ISI:000338747300001}},
}

@article{ ISI:000502686500003,
Author = {Hagen, Loni and Keller, Thomas E. and Yerden, Xiaoyi and Luna-Reyes,
   Luis Felipe},
Title = {{Open data visualizations and analytics as tools for policy-making}},
Journal = {{GOVERNMENT INFORMATION QUARTERLY}},
Year = {{2019}},
Volume = {{36}},
Number = {{4}},
Month = {{OCT}},
Abstract = {{Government agencies collect large amounts of structured and unstructured
   data. Although these data can be used to improve services as well as
   policy processes, it is not always clear how to analyze the data and how
   to glean insights for policy making, especially when the data includes
   large volumes of unstructured text data. This article reports opinions
   found in ``We the People{''} petition data using topic modeling and
   visual analytics. It provides an assessment of the usability of the
   visual analytics results for policy making based on interviews with data
   professionals and policy makers. We found that visual analytics have
   potentially positive impacts on policy making practices. Experts also
   articulated potential barriers regarding the adoption of visual
   analytics tools, and made suggestions. Potential barriers included
   insufficient resources in government agencies and difficulty integrating
   analytics with current work practices. The main suggestions involved
   providing training and interpretation guidelines along with the visual
   analytics tools. Major contributions of this study include: (1)
   suggesting viable visualization tools for analyzing textual data for
   policy making, and (2) suggesting how to lower barriers to adoption by
   increasing usability.}},
DOI = {{10.1016/j.giq.2019.06.004}},
Article-Number = {{101387}},
ISSN = {{0740-624X}},
EISSN = {{1872-9517}},
ResearcherID-Numbers = {{Luna-Reyes, Luis F./G-5548-2012
   Hagen, Loni/AAO-6618-2021}},
ORCID-Numbers = {{Hagen, Loni/0000-0002-6532-0852}},
Unique-ID = {{ISI:000502686500003}},
}

@inproceedings{ ISI:000459238600067,
Author = {Gervasoni, Luciano and Fenet, Serge and Perrier, Regis and Sturm, Peter},
Book-Group-Author = {{IEEE}},
Title = {{Convolutional neural networks for disaggregated population mapping using
   open data}},
Booktitle = {{2018 IEEE 5TH INTERNATIONAL CONFERENCE ON DATA SCIENCE AND ADVANCED
   ANALYTICS (DSAA)}},
Series = {{Proceedings of the International Conference on Data Science and Advanced
   Analytics}},
Year = {{2018}},
Pages = {{594-603}},
Note = {{5th IEEE International Conference on Data Science and Advanced Analytics
   (IEEE DSAA), Turin, ITALY, OCT 01-04, 2018}},
Organization = {{IEEE; ACM SIGKDD; Amer Stat Assoc; European Assoc Data Sci; IEEE
   Computat Intelligence Soc; ISI Fdn; NYU Stern Fubon Ctr; SoBigData;
   crowdAI; Intesa Sanpaolo}},
Abstract = {{High resolution population count data are vital for numerous
   applications such as urban planning, transportation model calibration,
   and population growth impact measurements, among others. In this work,
   we present and evaluate an end-to-end framework for computing
   disaggregated population mapping employing convolutional neural networks
   (CNNs). Using urban data extracted from the OpenStreetMap database, a
   set of urban features are generated which are used to guide population
   density estimates at a higher resolution.
   A population density grid at a 200 by 200 meter spatial resolution is
   estimated, using as input gridded population data of 1 by 1 kilometer.
   Our approach relies solely on open data with a wide geographical
   coverage, ensuring replicability and potential applicability to a great
   number of cities in the world. Fine-grained gridded population data is
   used for 15 French cities in order to train and validate our model. A
   stand-alone city is kept out for the validation procedure. The results
   demonstrate that the neural network approach using massive OpenStreetMap
   data outperforms other approaches proposed in related works.}},
DOI = {{10.1109/DSAA.2018.00076}},
ISSN = {{2472-1573}},
ISBN = {{978-1-5386-5090-5}},
ORCID-Numbers = {{Gervasoni, Luciano/0000-0002-3369-5959}},
Unique-ID = {{ISI:000459238600067}},
}

@article{ ISI:000637006400003,
Author = {Read, Kevin and Campbell, Alanna and Kitchin, Vanessa and MacDonald,
   Heather and McKeown, Sandra},
Title = {{Embracing the value of research data: Introducing the JCHLA/JABSC Data
   Sharing Policy}},
Journal = {{JOURNAL OF THE CANADIAN HEALTH LIBRARIES ASSOCIATION}},
Year = {{2021}},
Volume = {{42}},
Number = {{1}},
Pages = {{6-13}},
Month = {{APR}},
Abstract = {{Health sciences researchers are being asked to share their data more
   frequently due to funder policies, journal requirements, or interest
   from their peers. Health sciences librarians (HSLs) have simultaneously
   begun to provide support to researchers in this space through training,
   participating in RDM efforts on research grants, and developing
   comprehensive data services programs. If supporting researchers' data
   sharing efforts is a worthwhile investment for HSLs, it is crucial that
   we practice data sharing in our own research endeavours. Sharing data is
   a positive step in the right direction, as it can increase the
   transparency, reliability, and reusability of HSL-related research
   outputs. Furthermore, being able to identify and connect with
   researchers in relation to the challenges associated with data sharing
   can help HSLs empathize with their communities and gain new perspectives
   on improving support in this area. To that end, the Journal of the
   Canadian Health Libraries Association / Journal de l'Association des
   bibliotheques de la sante du Canada (JCHLA/JABSC) has developed a Data
   Sharing Policy to improve the transparency and reusability of research
   data underlying the results of its publications. This paper will
   describe the approach taken to inform and develop this policy.}},
DOI = {{10.29173/jchla29536}},
ISSN = {{1708-6892}},
ResearcherID-Numbers = {{MacDonald, Heather/H-1704-2014
   }},
ORCID-Numbers = {{MacDonald, Heather/0000-0001-9480-2677
   Read, Kevin/0000-0002-7511-9036
   McKeown, Sandra/0000-0002-2728-6702}},
Unique-ID = {{ISI:000637006400003}},
}

@article{ ISI:000360602000001,
Author = {Powers, Christina M. and Mills, Karmann A. and Morris, Stephanie A. and
   Klaessig, Fred and Gaheen, Sharon and Lewinski, Nastassja and Hendren,
   Christine Ogilvie},
Title = {{Nanocuration workflows: Establishing best practices for identifying,
   inputting, and sharing data to inform decisions on nanomaterials}},
Journal = {{BEILSTEIN JOURNAL OF NANOTECHNOLOGY}},
Year = {{2015}},
Volume = {{6}},
Pages = {{1860-1871}},
Month = {{SEP 4}},
Abstract = {{There is a critical opportunity in the field of nanoscience to compare
   and integrate information across diverse fields of study through
   informatics (i.e., nanoinformatics). This paper is one in a series of
   articles on the data curation process in nanoinformatics (nanocuration).
   Other articles in this series discuss key aspects of nanocuration
   (temporal metadata, data completeness, database integration), while the
   focus of this article is on the nanocuration workflow, or the process of
   identifying, inputting, and reviewing nanomaterial data in a data
   repository. In particular, the article discusses: 1) the rationale and
   importance of a defined workflow in nanocuration, 2) the influence of
   organizational goals or purpose on the workflow, 3) established workflow
   practices in other fields, 4) current workflow practices in
   nanocuration, 5) key challenges for workflows in emerging fields like
   nanomaterials, 6) examples to make these challenges more tangible, and
   7) recommendations to address the identified challenges. Throughout the
   article, there is an emphasis on illustrating key concepts and current
   practices in the field. Data on current practices in the field are from
   a group of stakeholders active in nanocuration. In general, the
   development of workflows for nanocuration is nascent, with few
   individuals formally trained in data curation or utilizing available
   nanocuration resources (e.g., ISA-TAB-Nano). Additional emphasis on the
   potential benefits of cultivating nanomaterial data via nanocuration
   processes (e.g., capability to analyze data from across research groups)
   and providing nanocuration resources (e.g., training) will likely prove
   crucial for the wider application of nanocuration workflows in the
   scientific community.}},
DOI = {{10.3762/bjnano.6.189}},
ISSN = {{2190-4286}},
ResearcherID-Numbers = {{Lewinski, Nastassja/E-2993-2012
   Hendren, Christine Ogilvie/AAM-3502-2021
   }},
ORCID-Numbers = {{Lewinski, Nastassja/0000-0002-9335-9949
   Hendren, Christine Ogilvie/0000-0002-9546-6545
   Mills, Karmann/0000-0002-3936-4253}},
Unique-ID = {{ISI:000360602000001}},
}

@inproceedings{ ISI:000452627900003,
Author = {Ramos, Esmeralda Florez},
Book-Group-Author = {{IEEE}},
Title = {{OPEN DATA DEVELOPMENT OF COUNTRIES: GLOBAL STATUS AND TRENDS}},
Booktitle = {{2017 ITU KALEIDOSCOPE: CHALLENGES FOR A DATA-DRIVEN SOCIETY (ITU K)}},
Year = {{2017}},
Note = {{ITU Kaleidoscope Conference - Challenges for a Data-Driven Society (ITU
   K), Nanjing, PEOPLES R CHINA, NOV 27-29, 2017}},
Organization = {{Ins Elect \& Elect Engineers; ITU Kaleidoscope; Jiangsu Inst Commun;
   Nanjing Fiberhome Starrysky; Nanjing Ironhorse Informat Technol; HBC;
   New H3C Technologies; IEEE Commun Soc; Int Conf Standardizat \& Innovat
   Informat Technol}},
Abstract = {{Open data plays a key role for governments strategy to deal with
   challenges of the future. It has the potential to improve public
   sector's transparency, engagement of civil society, and economic growth.
   This paper contributes to answering the questions: Can open data have an
   impact on innovation ? Under which condition is this the case ? Which
   data can be used to assess the progress on a country level ? Which
   countries are successful with open data ? How successful are the
   government actions to support economic development through open data ?
   The exploratory analysis investigates the relationship between open data
   readiness and measures on impact, and on changes in open data
   development level and the influence of the country's level of ICT
   development, transparency and freedom. This paper also takes a specific
   look at economic impact scores and their correlation with government
   initiatives for training and innovation on open data.
   It was found that success on open data at the country level is based on
   good levels of ICT development, freedom and in the interest of becoming
   more transparent. There are indications that countries with low ICT
   development do not profit from open data, but the evidence is limited,
   due to the small number of countries observed. There is a strong
   correlation between support for entrepreneurship \& business readiness
   and economic impact. However, the relationship between the development
   of these indicators during the time of the study and the measured impact
   is unclear.}},
ISBN = {{978-9-2612-4281-7}},
Unique-ID = {{ISI:000452627900003}},
}

@article{ ISI:000612547300003,
Author = {Kumar, Rajesh and Wang, WenYong and Kumar, Jay and Yang, Ting and Khan,
   Abdullah and Ali, Wazir and Ali, Ikram},
Title = {{An Integration of blockchain and AI for secure data sharing and
   detection of CT images for the hospitals}},
Journal = {{COMPUTERIZED MEDICAL IMAGING AND GRAPHICS}},
Year = {{2021}},
Volume = {{87}},
Month = {{JAN}},
Abstract = {{Deep learning, for image data processing, has been widely used to solve
   a variety of problems related to medical practices. However, researchers
   are constantly struggling to introduce ever efficient classification
   models. Recent studies show that deep learning can perform better and
   generalize well when trained using a large amount of data. Organizations
   such as hospitals, testing labs, research centers, etc. can share their
   data and collaboratively build a better learning model. Every
   organization wants to retain the privacy of their data, while on the
   other hand, these organizations want accurate and efficient learning
   models for various applications. The concern for privacy in medical data
   limits the sharing of data among multiple organizations due to some
   ethical and legal issues. To retain privacy and enable data sharing, we
   present a unique method that combines locally learned deep learning
   models over the blockchain to improve the prediction of lung cancer in
   health-care systems by filling the defined gap. There are several
   challenges involved in sharing that data while maintaining privacy. In
   this paper, we identify and address such challenges. The contribution of
   our work is four-fold: (i) We propose a method to secure medical data by
   only sharing the weights of the trained deep learning model via smart
   contract. (ii) To deal with different sized computed tomography (CT)
   images from various sources, we adopted the Bat algorithm and data
   augmentation to reduce the noise and overfitting for the global learning
   model. (iii) We distribute the local deep learning model wights to the
   blockchain decentralized network to train a global model. iv) We propose
   a recurrent convolutional neural network (RCNN) to estimate the region
   of interest (ROI) in theCT images. An extensive empirical study has been
   conducted to verify the significance of our proposed method for better
   prediction of cancer in the early stage. Experimental results of the
   proposed model can show that our proposed technique can detect the lung
   cancer nodules and also achieve better performance.}},
DOI = {{10.1016/j.compmedimag.2020.101812}},
Article-Number = {{101812}},
ISSN = {{0895-6111}},
EISSN = {{1879-0771}},
ResearcherID-Numbers = {{Ali, Wazir/AAK-7429-2021
   }},
ORCID-Numbers = {{Ali, Ikram/0000-0002-1499-9594
   Kumar, Rajesh/0000-0003-0813-7485}},
Unique-ID = {{ISI:000612547300003}},
}

@article{ ISI:000468210800018,
Author = {Zuo, Fan and Kurkcu, Abdullah and Ozbay, Kaan and Gao, Jingqin},
Title = {{Crowdsourcing Incident Information for Emergency Response using Open
   Data Sources in Smart Cities}},
Journal = {{TRANSPORTATION RESEARCH RECORD}},
Year = {{2018}},
Volume = {{2672}},
Number = {{1}},
Pages = {{198-208}},
Month = {{DEC}},
Abstract = {{Emergency events affect human security and safety as well as the
   integrity of the local infrastructure. Emergency response officials are
   required to make decisions using limited information and time. During
   emergency events, people post updates to social media networks, such as
   tweets, containing information about their status, help requests,
   incident reports, and other useful information. In this research
   project, the Latent Dirichlet Allocation (LDA) model is used to
   automatically classify incident-related tweets and incident types using
   Twitter data. Unlike the previous social media information models
   proposed in the related literature, the LDA is an unsupervised learning
   model which can be utilized directly without prior knowledge and
   preparation for data in order to save time during emergencies. Twitter
   data including messages and geolocation information during two recent
   events in New York City, the Chelsea explosion and Hurricane Sandy, are
   used as two case studies to test the accuracy of the LDA model for
   extracting incident-related tweets and labeling them by incident type.
   Results showed that the model could extract emergency events and
   classify them for both small and large-scale events, and the model's
   hyper-parameters can be shared in a similar language environment to save
   model training time. Furthermore, the list of keywords generated by the
   model can be used as prior knowledge for emergency event classification
   and training of supervised classification models such as support vector
   machine and recurrent neural network.}},
DOI = {{10.1177/0361198118798736}},
ISSN = {{0361-1981}},
EISSN = {{2169-4052}},
Unique-ID = {{ISI:000468210800018}},
}

@inproceedings{ ISI:000082355600004,
Author = {Epps, A and Bailey, G and Glatz, D},
Book-Group-Author = {{USENIX
   USENIX}},
Title = {{NFS and SMB data sharing within a heterogeneous environment: a real
   world study}},
Booktitle = {{PROCEEDINGS OF THE 2ND LARGE INSTALLATION SYSTEMS ADMINISTRATION OF
   WINDOWS NT CONFERENCE}},
Year = {{1999}},
Pages = {{37-41}},
Note = {{2nd Large Installation System Administration of Windows NT Conference,
   SEATTLE, WA, JUL 14-17, 1999}},
Organization = {{USENIX Assoc; SAGE}},
Abstract = {{A common problem encountered in a heterogeneous computing environment
   which includes both Unix \& PC/Windows hosts is sharing data between the
   different operating systems. Any approach must take into account the
   different methods of authenticating users, file permissions, and network
   protocols. With 2000 PC desktops \& 800 Unix users employing a variety
   of NT \% Unix servers, the Unix support group at the Color Printing and
   Imaging Division (CPID) of Tektronix needed a robust system that was
   inexpensive, easy to administer, simple and effective to use for both PC
   and Unix users. Administering separate installations of local NFS
   clients on the PCs had proven to be problematic, causing us to look at a
   centralized server-based solution that could provide native PC file
   sharing via the SMB protocol suite. The potential solutions we looked at
   were: TotalNet by Syntax (sold by Sun as SunPC), Samba v1.9.18p10,
   NetServices v.1 \& v.2 by Auspex, Network Appliance F230 series servers,
   and Sun's SunLink Server software v1 \& v1.1. We compared performance,
   ease of use by end users, ease of administration, cost, support,
   training, scalability, and ease of integration into our current
   environment. As of this writing the conclusion was to use Samba.}},
ISBN = {{1-880446-30-8}},
Unique-ID = {{ISI:000082355600004}},
}

@inproceedings{ ISI:000358309600028,
Author = {Balbo, S. and Boccardo, P. and Dalmasso, S. and Pasquali, P.},
Editor = {{Pirotti, F and Guarnieri, A and Vettore, A}},
Title = {{A PUBLIC PLATFORM FOR GEOSPATIAL DATA SHARING FOR DISASTER RISK
   MANAGEMENT}},
Booktitle = {{ROLE OF GEOMATICS IN HYDROGEOLOGICAL RISK}},
Series = {{International Archives of the Photogrammetry Remote Sensing and Spatial
   Information Sciences}},
Year = {{2013}},
Volume = {{40-5-W3}},
Pages = {{189-195}},
Note = {{International-Society-for-Photogrammetry-and-Remote-Sensing Workshop on
   The Role of Geomatics for Hydrogeological Risk, Univ Padua, Padua,
   ITALY, FEB 27-28, 2013}},
Organization = {{Int Soc Photogrammetry \& Remote Sensing}},
Abstract = {{Several studies have been conducted in Africa to assist local
   governments in addressing the risk situation related to natural hazards.
   Geospatial data containing information on vulnerability, impacts,
   climate change, disaster risk reduction is usually part of the output of
   such studies and is valuable to national and international organizations
   to reduce the risks and mitigate the impacts of disasters. Nevertheless
   this data isn't efficiently widely distributed and often resides in
   remote storage solutions hardly reachable.
   Spatial Data Infrastructures are technical solutions capable to solve
   this issue, by storing geospatial data and making them widely available
   through the internet. Among these solutions, GeoNode, an open source
   online platform for geospatial data sharing, has been developed in
   recent years. GeoNode is a platform for the management and publication
   of geospatial data. It brings together mature and stable open-source
   software projects under a consistent and easy-to-use interface allowing
   users, with little training, to quickly and easily share data and create
   interactive maps. GeoNode data management tools allow for integrated
   creation of data, metadata, and map visualizations. Each dataset in the
   system can be shared publicly or restricted to allow access to only
   specific users. Social features like user profiles and commenting and
   rating systems allow for the development of communities around each
   platform to facilitate the use, management, and quality control of the
   data the GeoNode instance contains (geonode.org).
   This paper presents a case study scenario of setting up a Web platform
   based on GeoNode. It is a public platform called MASDAP and promoted by
   the Government of Malawi in order to support development of the country
   and build resilience against natural disasters. A substantial amount of
   geospatial data has already been collected about hydrogeological risk,
   as well as several other-disasters related information. Moreover this
   platform will help to ensure that the data created by a number of past
   or ongoing projects is maintained and that this information remains
   accessible and useful. An Integrated Flood Risk Management Plan for a
   river basin has already been included in the platform and other data
   from future disaster risk management projects will be added as well.}},
DOI = {{10.5194/isprsarchives-XL-5-W3-189-2013}},
ISSN = {{2194-9034}},
ResearcherID-Numbers = {{BOCCARDO, PIERO/O-6359-2015
   Pirotti, Francesco/C-4906-2018}},
ORCID-Numbers = {{BOCCARDO, PIERO/0000-0003-4565-7332
   Pirotti, Francesco/0000-0002-4796-6406}},
Unique-ID = {{ISI:000358309600028}},
}

@inproceedings{ ISI:000451039800111,
Author = {Kreiser, Zachary and Killough, Brian and Rizvi, Syed R.},
Book-Group-Author = {{IEEE}},
Title = {{Water Across Synthetic Aperture Radar Data (WASARD): SAR Water Body
   Classification for the Open Data Cube}},
Booktitle = {{IGARSS 2018 - 2018 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING
   SYMPOSIUM}},
Series = {{IEEE International Symposium on Geoscience and Remote Sensing IGARSS}},
Year = {{2018}},
Pages = {{437-440}},
Note = {{38th IEEE International Geoscience and Remote Sensing Symposium
   (IGARSS), Valencia, SPAIN, JUL 22-27, 2018}},
Organization = {{Inst Elect \& Elect Engineers; Inst Elect \& Elect Engineers Geoscience
   \& Remote Sensing Soc; European Space Agcy}},
Abstract = {{The detection of inland water bodies from Synthetic Aperture Radar (SAR)
   data provides a great advantage over water detection with optical data,
   since SAR imaging is not impeded by cloud cover. Traditional methods of
   detecting water from SAR data involves using thresholding methods that
   can be labor intensive and imprecise. This paper describes Water Across
   Synthetic Aperture Radar Data (WASARD): a method of water detection from
   SAR data which automates and simplifies the thresholding process using
   machine learning on training data created from Geoscience Australia's
   WOFS algorithm. Of the machine learning models tested, the Linear
   Support Vector Machine was determined to be optimal, with the option of
   training using solely the VH polarization or a combination of the VH and
   VV polarizations. WASARD was able to identify water in the target area
   with a correlation of 97\% with WOFS.}},
ISSN = {{2153-6996}},
ISBN = {{978-1-5386-7150-4}},
Unique-ID = {{ISI:000451039800111}},
}

@article{ ISI:000396525500006,
Author = {de Wet, Febe and Kleynhans, Neil and van Compernolle, Dirk and
   Sahraeian, Reza},
Title = {{Speech recognition for under-resourced languages: Data sharing in hidden
   Markov model systems}},
Journal = {{SOUTH AFRICAN JOURNAL OF SCIENCE}},
Year = {{2017}},
Volume = {{113}},
Number = {{1-2}},
Pages = {{25-33}},
Month = {{JAN-FEB}},
Abstract = {{For purposes of automated speech recognition in under-resourced
   environments, techniques used to share acoustic data between closely
   related or similar languages become important. Donor languages with
   abundant resources can potentially be used to increase the recognition
   accuracy of speech systems developed in the resource poor target
   language. The assumption is that adding more data will increase the
   robustness of the statistical estimations captured by the acoustic
   models. In this study we investigated data sharing between Afrikaans and
   Flemish - an under-resourced and well-resourced language, respectively.
   Our approach was focused on the exploration of model adaptation and
   refinement techniques associated with hidden Markov model based speech
   recognition systems to improve the benefit of sharing data.
   Specifically, we focused on the use of currently available techniques,
   some possible combinations and the exact utilisation of the techniques
   during the acoustic model development process. Our findings show that
   simply using normal approaches to adaptation and refinement does not
   result in any benefits when adding Flemish data to the Afrikaans
   training pool. The only observed improvement was achieved when
   developing acoustic models on all available data but estimating model
   refinements and adaptations on the target data only.
   Significance:
   Acoustic modelling for under-resourced languages
   Automatic speech recognition for Afrikaans
   Data sharing between Flemish and Afrikaans to improve acoustic modelling
   for Afrikaans}},
DOI = {{10.17159/sajs.2017/20160038}},
Article-Number = {{2016-0038}},
ISSN = {{0038-2353}},
EISSN = {{1996-7489}},
ORCID-Numbers = {{de Wet, Febe/0000-0003-3495-9802}},
Unique-ID = {{ISI:000396525500006}},
}

@article{ ISI:000260487800001,
Author = {Tomlinson, Chris and Thimma, Manjula and Alexandrakis, Stelios and
   Castillo, Tito and Dennis, Jayne L. and Brooks, Anthony and Bradley,
   Thomas and Turnbull, Carly and Blaveri, Ekaterini and Barton, Geraint
   and Chiba, Norie and Maratou, Klio and Soutter, Pat and Aitman, Tim and
   Game, Laurence},
Title = {{MiMiR - an integrated platform for microarray data sharing, mining and
   analysis}},
Journal = {{BMC BIOINFORMATICS}},
Year = {{2008}},
Volume = {{9}},
Month = {{SEP 18}},
Abstract = {{Background: Despite considerable efforts within the microarray community
   for standardising data format, content and description, microarray
   technologies present major challenges in managing, sharing, analysing
   and re-using the large amount of data generated locally or
   internationally. Additionally, it is recognised that inconsistent and
   low quality experimental annotation in public data repositories
   significantly compromises the re-use of microarray data for
   meta-analysis. MiMiR, the Microarray data Mining Resource was designed
   to tackle some of these limitations and challenges. Here we present new
   software components and enhancements to the original infrastructure that
   increase accessibility, utility and opportunities for large scale mining
   of experimental and clinical data.
   Results: A user friendly Online Annotation Tool allows researchers to
   submit detailed experimental information via the web at the time of data
   generation rather than at the time of publication. This ensures the easy
   access and high accuracy of meta-data collected. Experiments are
   programmatically built in the MiMiR database from the submitted
   information and details are systematically curated and further annotated
   by a team of trained annotators using a new Curation and Annotation
   Tool. Clinical information can be annotated and coded with a clinical
   Data Mapping Tool within an appropriate ethical framework. Users can
   visualise experimental annotation, assess data quality, download and
   share data via a web-based experiment browser called MiMiR Online. All
   requests to access data in MiMiR are routed through a sophisticated
   middleware security layer thereby allowing secure data access and
   sharing amongst MiMiR registered users prior to publication. Data in
   MiMiR can be mined and analysed using the integrated EMAAS open source
   analysis web portal or via export of data and meta-data into Rosetta
   Resolver data analysis package.
   Conclusion: The new MiMiR suite of software enables systematic and
   effective capture of extensive experimental and clinical information
   with the highest MIAME score, and secure data sharing prior to
   publication. MiMiR currently contains more than 150 experiments
   corresponding to over 3000 hybridisations and supports the Microarray
   Centre's large microarray user community and two international
   consortia. The MiMiR flexible and scalable hardware and software
   architecture enables secure warehousing of thousands of datasets,
   including clinical studies, from microarray and potentially other -omics
   technologies.}},
DOI = {{10.1186/1471-2105-9-379}},
Article-Number = {{379}},
ISSN = {{1471-2105}},
ResearcherID-Numbers = {{Castillo, Fortunato/F-1191-2010
   /AAD-2166-2020
   }},
ORCID-Numbers = {{Maratou, Klio/0000-0001-6061-2322}},
Unique-ID = {{ISI:000260487800001}},
}

@article{ ISI:000621061800006,
Author = {Knapp, O. and Cerri, O. and Dissertori, G. and Nguyen, T. Q. and
   Pierini, M. and Vlimant, J. R.},
Title = {{Adversarially Learned Anomaly Detection on CMS open data: re-discovering
   the top quark}},
Journal = {{EUROPEAN PHYSICAL JOURNAL PLUS}},
Year = {{2021}},
Volume = {{136}},
Number = {{2}},
Month = {{FEB 19}},
Abstract = {{We apply an Adversarially Learned Anomaly Detection (ALAD) algorithm to
   the problem of detecting new physics processes in proton-proton
   collisions at the Large Hadron Collider. Anomaly detection based on ALAD
   matches performances reached by Variational Autoencoders, with a
   substantial improvement in some cases. Training the ALAD algorithm on
   4.4 fb-1 of 8 TeV CMS Open Data, we show how a data-driven anomaly
   detection and characterization would work in real life, re-discovering
   the top quark by identifying the main features of the tt<mml:mo
   stretchy={''}false{''}><overbar></mml:mover> experimental signature at
   the LHC.}},
DOI = {{10.1140/epjp/s13360-021-01109-4}},
Article-Number = {{236}},
ISSN = {{2190-5444}},
Unique-ID = {{ISI:000621061800006}},
}

@article{ ISI:000630285500001,
Author = {Gawas, Mahadev and Patil, Hemprasad and Govekar, Sweta S.},
Title = {{An integrative approach for secure data sharing in vehicular edge
   computing using Blockchain}},
Journal = {{PEER-TO-PEER NETWORKING AND APPLICATIONS}},
Abstract = {{The rapid growth and technological progressions in the vehicle edge
   computing and networks (VECON) enhanced existing vehicular
   administrations through information sharing and information
   investigation, which further incremented traffic security difficulties.
   This carries the need to verify vehicle networks with gigantic
   information stockpiling substances. Recognizing the vehicles that
   communicate fashioned messages and ensuring the protection of every
   vehicle is an essential assignment. Blockchain can be utilized as an
   effective solution to provide security during vehicle edge computing.
   Every transaction and data is recorded in the blockchain, which expands
   the number of blocks after some time. Another test in blockchain methods
   is utilizing a consensus mechanism, which can be effectively undermined
   by the attackers. Artificial intelligence (AI) trained by machine
   learning (ML) algorithms as an amazing paradigm can be incorporated with
   blockchain to settle these issues. The data storing methods of
   blockchain can likewise be enhanced with the assistance of ML
   algorithms. In this paper, a two-tier authenticated consortium
   blockchain (TTA-CB) protocol is proposed for secure information
   partaking in Vehicle Edge Computing and Networks (VECONs). Utilizing a
   one-time password (OTP) based notoriety estimation calculation, the
   misbehaving vehicles are recognized. The use of Particle Swarm
   Optimization (PSO) understands the ideal data provider selection issue
   utilizing notoriety esteems. Exploratory outcomes demonstrate the
   importance of the proposed strategy, and the correlation results
   demonstrate that the proposed technique is unrivaled and secure.}},
DOI = {{10.1007/s12083-021-01107-4}},
Early Access Date = {{MAR 2021}},
ISSN = {{1936-6442}},
EISSN = {{1936-6450}},
ResearcherID-Numbers = {{Gawas, Mahadev/AAK-9014-2021}},
ORCID-Numbers = {{Gawas, Mahadev/0000-0001-9933-2934}},
Unique-ID = {{ISI:000630285500001}},
}

@inproceedings{ ISI:000485630704080,
Author = {Najjar, Alameen and Kaneko, Shun'ichi and Miyanaga, Yoshikazu},
Book-Group-Author = {{AAAI}},
Title = {{Combining Satellite Imagery and Open Data to Map Road Safety}},
Booktitle = {{THIRTY-FIRST AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE}},
Year = {{2017}},
Pages = {{4524-4530}},
Note = {{31st AAAI Conference on Artificial Intelligence, San Francisco, CA, FEB
   04-09, 2017}},
Organization = {{Assoc Advancement Artificial Intelligence}},
Abstract = {{Improving road safety is critical for the sustainable development of
   cities. A road safety map is a powerful tool that can help prevent
   future traffic accidents. However, accurate mapping requires accurate
   data collection, which is both expensive and labor intensive. Satellite
   imagery is increasingly becoming abundant, higher in resolution and
   affordable. Given the recent successes deep learning has achieved in the
   visual recognition field, we are interested in investigating whether it
   is possible to use deep learning to accurately predict road safety
   directly from raw satellite imagery. To this end, we propose a deep
   learning-based mapping approach that leverages open data to learn from
   raw satellite imagery robust deep models able to predict accurate
   city-scale road safety maps at an affordable cost. To empirically
   validate the proposed approach, we trained a deep model on satellite
   images obtained from over 647 thousand traffic-accident reports
   collected over a period of four years by the New York city Police
   Department. The best model predicted road safety from raw satellite
   imagery with an accuracy of 78\%. We also used the New York city model
   to predict for the city of Denver a city-scale map indicating road
   safety in three levels. Compared to a map made from three years' worth
   of data collected by the Denver city Police Department, the map
   predicted from raw satellite imagery has an accuracy of 73\%.}},
ResearcherID-Numbers = {{Miyanaga, Yoshikazu/D-6140-2012}},
Unique-ID = {{ISI:000485630704080}},
}

@article{ ISI:000458883000005,
Author = {Sandhaus, Shana and Kaufmann, Dorsey and Ramirez-Andreotta, Monica},
Title = {{Public participation, trust and data sharing: gardens as hubs for
   citizen science and environmental health literacy efforts}},
Journal = {{INTERNATIONAL JOURNAL OF SCIENCE EDUCATION PART B-COMMUNICATION AND
   PUBLIC ENGAGEMENT}},
Year = {{2019}},
Volume = {{9}},
Number = {{1}},
Pages = {{54-71}},
Month = {{JAN 2}},
Abstract = {{Gardenroots: A Citizen Science Project (2015) is the product of a needs
   assessment, revealing environmental quality concerns of gardeners living
   near hazardous waste or resource extraction activities. Participants
   were trained, collected garden samples for analysis, and later received
   their data visualized (individual and aggregated) via community events
   or mail. This article describes participant motivations, changes in
   knowledge and efficacy, and whether these depend on the mode of data
   sharing and visualization. Motivations were internal, and self-efficacy
   increased, while knowledge and satisfaction were higher in event
   attendees due to increased researcher contact. This reveals importance
   of data-sharing events, data visualizations, and participatory research
   processes.}},
DOI = {{10.1080/21548455.2018.1542752}},
ISSN = {{2154-8455}},
EISSN = {{2154-8463}},
ORCID-Numbers = {{Ramirez-Andreotta, Monica/0000-0001-6220-5763}},
Unique-ID = {{ISI:000458883000005}},
}

@article{ ISI:000609999200039,
Author = {Poppiel, Raul Roberto and Melo Dematte, Jose Alexandre and Rosin,
   Nicolas Augusto and Campos, Lucas Rabelo and Tayebi, Mahboobeh and
   Bonfatti, Benito Roberto and Ayoubi, Shamsollah and Tajik, Samaneh and
   Afshar, Farideh Abbaszadeh and Jafari, Azam and Hamzehpour, Nikou and
   Taghizadeh-Mehrjardi, Ruhollah and Ostovari, Yaser and Asgari, Najmeh
   and Naimi, Salman and Nabiollahi, Kamal and Fathizad, Hassan and
   Zeraatpisheh, Mojtaba and Javaheri, Fatemeh and Doustaky, Maryam and
   Naderi, Mehdi and Dehghani, Somayeh and Atash, Saeedeh and Farshadirad,
   Akram and Mirzaee, Salman and Shahriari, Ali and Ghorbani, Maryam and
   Rahmati, Mehdi},
Title = {{High resolution middle eastern soil attributes mapping via open data and
   cloud computing}},
Journal = {{GEODERMA}},
Year = {{2021}},
Volume = {{385}},
Month = {{MAR 1}},
Abstract = {{Soil presents a high vulnerability to the environmental degradation
   processes especially in arid and semiarid regions, requiring research
   that leads to its understanding. To date, there are no detailed soil
   maps covering a large extension of the Middle East region, especially
   for calcium carbonate content. Thus, we used topsoil data (0-20 cm) from
   more than 5000 sites for mapping near 3,338,000 square km of the Middle
   East. To do this, we used covariates obtained from remote sensing data
   and random forest (RF) algorithm. Around 65\% of the soil information
   was acquired from Iranian datasets and the remaining from the World Soil
   Information Service dataset. By using 30 covariates layers-soil,
   climate, relief, parent material and age features- we then trained and
   tuned RF regression models-in R software- and used the optimal ones
   (according to the minimum root mean square error) for making spatial
   predictions-within Google Earth Engine- of topsoil attributes and
   associated uncertainties at 30 m resolution. All covariates were
   relatively important for mapping topsoil attributes, ranging from 4\% to
   98\%. Annual precipitation, temperature annual range and elevation were
   the most important ones (>31\%). Overall, the prediction models trained
   by RF explained around 40-66\% of the variation present in topsoil
   attributes. The ratio of the performance to interquartile distance
   (RPIQ) ranged between 1.59 and 2.83, suggesting accurate models. Our
   predicted maps indicated that sandy and loamy soils with poor organic
   carbon levels, alkaline reaction and high calcium carbonate content were
   widespread in middle eastern topsoils. Our framework overcomes some
   limitations related to high computational requirements and enables
   accurate predictions of topsoil attributes. Our maps presented correct
   pedological correspondences and had realistic spatial representations
   and interesting levels of uncertainties.}},
DOI = {{10.1016/j.geoderma.2020.114890}},
Article-Number = {{114890}},
ISSN = {{0016-7061}},
EISSN = {{1872-6259}},
ResearcherID-Numbers = {{Jafari, Azam/R-7884-2017
   Taghizadeh-Mehrjardi, Ruhollah/H-3682-2013
   Poppiel, Raul Roberto/R-8128-2019
   }},
ORCID-Numbers = {{Jafari, Azam/0000-0003-0800-2812
   Taghizadeh-Mehrjardi, Ruhollah/0000-0002-4620-6624
   Rosin, Nicolas Augusto/0000-0002-3439-4701
   dehghani, somayeh/0000-0002-7736-8335
   Poppiel, Raul Roberto/0000-0002-1628-4154
   Zeraatpisheh, Mojtaba/0000-0001-7209-0744}},
Unique-ID = {{ISI:000609999200039}},
}

@article{ ISI:000345529200105,
Author = {Cases, Montserrat and Briggs, Katharine and Steger-Hartmann, Thomas and
   Pognan, Francois and Marc, Philippe and Kleinoeder, Thomas and Schwab,
   Christof H. and Pastor, Manuel and Wichard, Joerg and Sanz, Ferran},
Title = {{The eTOX Data-Sharing Project to Advance in Silico Drug-Induced Toxicity
   Prediction}},
Journal = {{INTERNATIONAL JOURNAL OF MOLECULAR SCIENCES}},
Year = {{2014}},
Volume = {{15}},
Number = {{11}},
Pages = {{21136-21154}},
Month = {{NOV}},
Abstract = {{The high-quality in vivo preclinical safety data produced by the
   pharmaceutical industry during drug development, which follows numerous
   strict guidelines, are mostly not available in the public domain. These
   safety data are sometimes published as a condensed summary for the few
   compounds that reach the market, but the majority of studies are never
   made public and are often difficult to access in an automated way, even
   sometimes within the owning company itself. It is evident from many
   academic and industrial examples, that useful data mining and model
   development requires large and representative data sets and careful
   curation of the collected data. In 2010, under the auspices of the
   Innovative Medicines Initiative, the eTOX project started with the
   objective of extracting and sharing preclinical study data from paper or
   pdf archives of toxicology departments of the 13 participating
   pharmaceutical companies and using such data for establishing a
   detailed, well-curated database, which could then serve as source for
   read-across approaches (early assessment of the potential toxicity of a
   drug candidate by comparison of similar structure and/or effects) and
   training of predictive models. The paper describes the efforts
   undertaken to allow effective data sharing intellectual property (IP)
   protection and set up of adequate controlled vocabularies) and to
   establish the database (currently with over 4000 studies contributed by
   the pharma companies corresponding to more than 1400 compounds). In
   addition, the status of predictive models building and some specific
   features of the eTOX predictive system (eTOXsys) are presented as
   decision support knowledge-based tools for drug development process at
   an early stage.}},
DOI = {{10.3390/ijms151121136}},
ISSN = {{1422-0067}},
ResearcherID-Numbers = {{MARC, Philippe/E-2896-2015
   MARC, Philippe/AAB-1337-2019
   Gasull, Martina/J-4076-2019
   Sanz, Ferran/B-3852-2009
   Pastor, Manuel/A-1566-2010}},
ORCID-Numbers = {{MARC, Philippe/0000-0002-0064-0572
   MARC, Philippe/0000-0002-0064-0572
   Sanz, Ferran/0000-0002-7534-7661
   Pastor, Manuel/0000-0001-8850-1341}},
Unique-ID = {{ISI:000345529200105}},
}

@inproceedings{ ISI:000432711502086,
Author = {Charalabidis, Yannis and Alexopoulos, Charalampos and Diamantopoulou,
   Vasiliki and Androutsopoulou, Aggeliki},
Editor = {{Bui, TX and Sprague, RH}},
Title = {{An open data and open services repository for supporting citizen-driven
   application development for governance}},
Booktitle = {{PROCEEDINGS OF THE 49TH ANNUAL HAWAII INTERNATIONAL CONFERENCE ON SYSTEM
   SCIENCES (HICSS 2016)}},
Series = {{Proceedings of the Annual Hawaii International Conference on System
   Sciences}},
Year = {{2016}},
Pages = {{2596-2604}},
Note = {{49th Hawaii International Conference on System Sciences (HICSS), Koloa,
   HI, JAN 05-08, 2016}},
Organization = {{Pacific Res Inst Informat Syst \& Management; Univ Hawaii Manoa, Shidler
   Coll Business, Dept IT Management; IBM; Provalis Res; Int Soc Serv
   Innovat Profess; Teradata; Univ Network; IEEE Comp Soc}},
Abstract = {{Open data portals have been a primary source for publishing datasets
   from various sectors of administration, all over the world. However,
   making open data available does not necessarily lead to better
   utilisation from citizens and businesses. Our paper presents a new
   framework and a prototype system for supporting open application
   development by citizen communities, through gathering and making
   available open data and open web services sources from governmental
   actors, combined with an application development environment, training
   material and application examples.}},
DOI = {{10.1109/HICSS.2016.325}},
ISSN = {{1060-3425}},
ISBN = {{978-0-7695-5670-3}},
ResearcherID-Numbers = {{Diamantopoulou, Vasiliki/H-2163-2013
   Charalabidis, Yannis/AAL-7256-2021
   Alexopoulos, Charalampos/AAH-6284-2021}},
ORCID-Numbers = {{Diamantopoulou, Vasiliki/0000-0002-9398-2415
   Alexopoulos, Charalampos/0000-0002-6610-0675}},
Unique-ID = {{ISI:000432711502086}},
}

@article{ ISI:000510953900010,
Author = {Zhao, Jiahui and Fan (David), Wei and Zhai, Xuehao},
Title = {{Identification of land-use characteristics using bicycle sharing data: A
   deep learning approach}},
Journal = {{JOURNAL OF TRANSPORT GEOGRAPHY}},
Year = {{2020}},
Volume = {{82}},
Month = {{JAN}},
Abstract = {{Extensive research has shown that urban land-use characteristics,
   including resident, work, consumption, transit, etc., are significantly
   interrelated with travel behaviors and travel demands. Many research
   efforts have been made to evaluate the impact of land use planning or
   policies on travel behavior, however, few studies are able to
   quantitatively measure the land-use characteristics based on the data of
   travel behaviors or travel demand. In this paper, a new hybrid model
   that combines time series feature extraction and deep neural network is
   proposed to identify regional land use characteristics and quantify land
   use intensity using ridership data of bicycle sharing. This method
   consists of four main parts: (i) A set of land-use characteristic labels
   are evaluated based on planning and Geographic Information System (GIS)
   data. (ii) An ensemble clustering method is used to determine the
   segmentation points of ridership time series. (iii) The statistical
   characteristics of the segmented time series are extracted and used as
   input to the neural network. (iv) A deep neural network is established
   and trained based on the processed ridership features and land-use
   labels. In terms of data collection, ridership data of the
   bicycle-sharing parking spots and land-use planning data are obtained
   from bicycle-sharing system and planning department in San Francisco Bay
   Area, California U.S.A., respectively. The test results show that this
   approach has high accuracy for identifying land-use characteristics
   based on several standard evaluation measures and that the
   identification distribution can be well explained. The extension results
   further prove that the model can be applied to effectively analyze the
   main land-use characteristics of the region although the identification
   results may become unstable after 3-4 months.}},
DOI = {{10.1016/j.jtrangeo.2019.102562}},
Article-Number = {{102562}},
ISSN = {{0966-6923}},
EISSN = {{1873-1236}},
ORCID-Numbers = {{Zhao, Jiahui/0000-0001-9484-8165}},
Unique-ID = {{ISI:000510953900010}},
}

@inproceedings{ ISI:000460577800055,
Author = {Carbone, Raffaella and Fortunato, Giovanni and Pace, Giovanna and
   Pastore, Emanuele and Pietragalla, Luciana and Postiglione, Lydia and
   Scorza, Francesco},
Editor = {{Gervasi, O and Murgante, B and Misra, S and Stankova, E and Torre, CM and Rocha, AMAC and Taniar, D and Apduhan, BO and Tarantino, E and Ryu, Y}},
Title = {{Using Open Data and Open Tools in Defining Strategies for the
   Enhancement of Basilicata Region}},
Booktitle = {{COMPUTATIONAL SCIENCE AND ITS APPLICATIONS - ICCSA 2018, PT V}},
Series = {{Lecture Notes in Computer Science}},
Year = {{2018}},
Volume = {{10964}},
Number = {{V}},
Pages = {{725-733}},
Note = {{18th International Conference on Computational Science and Its
   Applications (ICCSA), Monash Univ, Caulfield Campus, Melbourne,
   AUSTRALIA, JUL 02-05, 2018}},
Organization = {{Univ Perugia; Kyushu Sangyo Univ; Univ Basilicata; Univ Minho; Springer
   Int Publishing AG}},
Abstract = {{Open data availability, participation and knowledge sharing are becoming
   increasingly important in planning processes aimed at protecting and
   enhancing the territory. This paper presents an application of
   Volunteered Geographic Information (VGI) for the creation of an open
   database for the enhancement of Basilicata region territory. The work
   was carried out during the Smart Basilicata training project and led to
   the definition of a map of the services of the regional territory,
   starting from open source tools and data available online and processed
   through geographic information systems.}},
DOI = {{10.1007/978-3-319-95174-4\_55}},
ISSN = {{0302-9743}},
EISSN = {{1611-3349}},
ISBN = {{978-3-319-95174-4; 978-3-319-95173-7}},
ResearcherID-Numbers = {{Scorza, Francesco/J-5932-2019
   }},
ORCID-Numbers = {{Scorza, Francesco/0000-0001-6149-7346
   Fortunato, Giovanni/0000-0002-0928-9387}},
Unique-ID = {{ISI:000460577800055}},
}

@article{ ISI:000453495900007,
Author = {Musto, Cataldo and Narducci, Fedelucio and Lops, Pasquale and de Gemmis,
   Marco and Semeraro, Giovanni},
Title = {{Linked open data-based explanations for transparent recommender systems}},
Journal = {{INTERNATIONAL JOURNAL OF HUMAN-COMPUTER STUDIES}},
Year = {{2019}},
Volume = {{121}},
Pages = {{93-107}},
Month = {{JAN}},
Abstract = {{In this article we propose a framework that generates natural language
   explanations supporting the suggestions generated by a recommendation
   algorithm.
   The cornerstone of our approach is the usage of Linked Open Data (LOD)
   for explanation aims. Indeed, the descriptive properties freely
   available in the LOD cloud (e.g., the author of a book or the director
   of a movie) can be used to build a graph that connects the
   recommendations the user received to the items she previously liked via
   the properties extracted from the LOD cloud. In a nutshell, our approach
   is based on the insight that properties describing the items the user
   previously liked as well as the suggestions she received can be
   effectively used to explain the recommendations.
   Such a framework is both algorithm-independent and domain-independent,
   thus it can generate a natural language explanation for every kind of
   recommendation algorithm, and it can be used to explain a single
   recommendation (Top-1 scenario) as well as a group of recommendations
   (Top-N scenario). It is worth noting that the algorithm-independent
   characteristic does not mean that the framework is able to explain to
   the user how the recommendations have been generated and how the
   recommendation algorithm works. The framework explains to users why they
   might like the recommended items, independently from the recommendation
   algorithm that generated the recommendations.
   In the experimental evaluation, we carried out a user study (N = 680)
   aiming to investigate the effectiveness of our framework in three
   different domains, as movies, books and music. Results showed that our
   technique leads to transparent explanations for all the domains, and
   such explanations resulted independent of the specific recommendation
   algorithm in most of the experimental settings. Moreover, we also showed
   the goodness of our strategy when an entire group of recommendations has
   to be explained.
   As a case study, we integrated the framework in a real-world
   application, a conversational recommender system implemented as a
   Telegram Bot. The idea is to use the explanation for supporting both the
   training phase (when the user expresses her preferences) and the
   recommendation step (when the user receives the recommendations).
   Interesting outcomes emerge from these preliminary experiments. (C) 2018
   Elsevier Ltd. All rights reserved.}},
DOI = {{10.1016/j.ijhcs.2018.03.003}},
ISSN = {{1071-5819}},
EISSN = {{1095-9300}},
ResearcherID-Numbers = {{Semeraro, Giovanni/AAC-2156-2020
   }},
ORCID-Numbers = {{Semeraro, Giovanni/0000-0001-6883-1853
   narducci, fedelucio/0000-0002-9255-3256
   Musto, Cataldo/0000-0001-6089-928X}},
Unique-ID = {{ISI:000453495900007}},
}

@article{ ISI:000459208000004,
Author = {Kim, Soo Young and Yi, Hyun Jung and Huh, Sun},
Title = {{Current and planned adoption of data sharing policies by editors of
   Korean scholarly journals}},
Journal = {{SCIENCE EDITING}},
Year = {{2019}},
Volume = {{6}},
Number = {{1}},
Pages = {{19-24}},
Month = {{FEB}},
Abstract = {{Purpose: This study analyzed the present status of data sharing polices
   and attitudes towards such policies through a web-based survey of
   editors of scholarly journals published in Korea.
   Methods: From December 26, 2018 to January 3, 2019, a survey was
   distributed to 1,055 persons listed in the member directories of both
   the Korean Council of Science Editors and the Korean Federation of
   Science \& Technology Societies. The survey contained four items on
   subjects information, three items that gathered information about the
   journals, and two further items on reasons for adopting or not adopting
   a data sharing policy and further opinions about such policies.
   Results: Of the 100 respondents (from 100 journals), 13 stated that
   their journals had already adopted a data sharing policy. The strength
   of the policy was recommendation-only in 10 of those 13 journals. The
   most frequent reason for adopting a data sharing policy was to follow
   international trends. The repository sites were the Harvard Dataverse
   for two journals and Mendeley Data for one. The most common reasons for
   not adopting a data sharing policy were a lack of knowledge on data
   sharing, the possibility that submitters would not want to share their
   data, and the questionable effect of data sharing on scientific
   development.
   Conclusion: Data sharing policies were uncommon among Korean scholarly
   journals. The advantages and disadvantages of adopting such policies
   should be discussed more actively among editors and researchers.
   Furthermore, data sharing infrastructure and training courses are
   required for data sharing policies to be established in scholarly
   journals in Korea.}},
DOI = {{10.6087/kcse.151}},
ISSN = {{2288-7474}},
EISSN = {{2288-8063}},
ResearcherID-Numbers = {{Huh, Sun/A-3068-2011
   }},
ORCID-Numbers = {{Huh, Sun/0000-0002-8559-8640
   Yi, Hyun Jung/0000-0003-0663-4373}},
Unique-ID = {{ISI:000459208000004}},
}

@article{ ISI:000477874300007,
Author = {Quinn, Martha and Forman, Jane and Harrod, Molly and Winter, Suzanne and
   Fowler, Karen E. and Krein, Sarah L. and Gupta, Ashwin and Saint, Sanjay
   and Singh, Hardeep and Chopra, Vineet},
Title = {{Electronic health records, communication, and data sharing: challenges
   and opportunities for improving the diagnostic process}},
Journal = {{DIAGNOSIS}},
Year = {{2019}},
Volume = {{6}},
Number = {{3}},
Pages = {{241-248}},
Month = {{SEP}},
Abstract = {{Background: Diagnosis requires that clinicians communicate and share
   patient information in an efficient manner. Advances in electronic
   health records (EHRs) and health information technologies have created
   both challenges and opportunities for such communication.
   Methods: We conducted a multi-method, focused ethnographic study of
   physicians on general medicine inpatient units in two teaching
   hospitals. Physician teams were observed during and after morning rounds
   to understand workflow, data sharing and communication during diagnosis.
   To validate findings, interviews and focus groups were conducted with
   physicians. Field notes and interview/focus group transcripts were
   reviewed and themes identified using content analysis.
   Results: Existing communication technologies and EHR-based data sharing
   processes were perceived as barriers to diagnosis. In particular,
   reliance on paging systems and lack of face-to-face communication among
   clinicians created obstacles to sustained thinking and discussion of
   diagnostic decision-making. Further, the EHR created data overload and
   data fragmentation, making integration for diagnosis difficult. To
   improve diagnosis, physicians recommended replacing pagers with two-way
   communication devices, restructuring the EHR to facilitate access to key
   information and improving training on EHR systems.
   Conclusions: As advances in health information technology evolve,
   challenges in the way clinicians share information during the diagnostic
   process will rise. To improve diagnosis, changes to both the technology
   and the way in which we use it may be necessary.}},
DOI = {{10.1515/dx-2018-0036}},
ISSN = {{2194-8011}},
EISSN = {{2194-802X}},
ResearcherID-Numbers = {{Fowler, Karen/B-5383-2015
   Saint, Sanjay/AAF-5126-2019}},
ORCID-Numbers = {{Fowler, Karen/0000-0001-8274-8407
   }},
Unique-ID = {{ISI:000477874300007}},
}

@article{ ISI:000618224700001,
Author = {Reinertsen, Ingerid and Collins, D. Louis and Drouin, Simon},
Title = {{The Essential Role of Open Data and Software for the Future of
   Ultrasound-Based Neuronavigation}},
Journal = {{FRONTIERS IN ONCOLOGY}},
Year = {{2021}},
Volume = {{10}},
Month = {{FEB 2}},
Abstract = {{With the recent developments in machine learning and modern graphics
   processing units (GPUs), there is a marked shift in the way
   intra-operative ultrasound (iUS) images can be processed and presented
   during surgery. Real-time processing of images to highlight important
   anatomical structures combined with in-situ display, has the potential
   to greatly facilitate the acquisition and interpretation of iUS images
   when guiding an operation. In order to take full advantage of the recent
   advances in machine learning, large amounts of high-quality annotated
   training data are necessary to develop and validate the algorithms. To
   ensure efficient collection of a sufficient number of patient images and
   external validity of the models, training data should be collected at
   several centers by different neurosurgeons, and stored in a standard
   format directly compatible with the most commonly used machine learning
   toolkits and libraries. In this paper, we argue that such effort to
   collect and organize large-scale multi-center datasets should be based
   on common open source software and databases. We first describe the
   development of existing open-source ultrasound based neuronavigation
   systems and how these systems have contributed to enhanced neurosurgical
   guidance over the last 15 years. We review the impact of the large
   number of projects worldwide that have benefited from the publicly
   available datasets ``Brain Images of Tumors for Evaluation{''} (BITE)
   and ``Retrospective evaluation of Cerebral Tumors{''} (RESECT) that
   include MR and US data from brain tumor cases. We also describe the need
   for continuous data collection and how this effort can be organized
   through the use of a well-adapted and user-friendly open-source software
   platform that integrates both continually improved guidance and
   automated data collection functionalities.}},
DOI = {{10.3389/fonc.2020.619274}},
Article-Number = {{619274}},
ISSN = {{2234-943X}},
Unique-ID = {{ISI:000618224700001}},
}

@inproceedings{ ISI:000380406200019,
Author = {Peng, Yung-Hsing and Hsu, Chin-Shun and Huang, Po-Chuang},
Book-Group-Author = {{IEEE}},
Title = {{Developing Crop Price Forecasting Service Using Open Data from Taiwan
   Markets}},
Booktitle = {{2015 CONFERENCE ON TECHNOLOGIES AND APPLICATIONS OF ARTIFICIAL
   INTELLIGENCE (TAAI)}},
Year = {{2015}},
Pages = {{172-175}},
Note = {{Conference on Technologies and Applications of Artificial Intelligence
   (TAAI), Tainan, TAIWAN, NOV 20-22, 2015}},
Organization = {{IEEE; NUTN; KUAS; INC Most; IEEE Computat Inteliigence Soc; TCGA;
   Sigmobile Taiwan; IEEE CIS Tainan Chapter; KIIS}},
Abstract = {{From the perspective of agricultural business, the market price of
   certain crop reflects the demand of that crop in current stage.
   Therefore, to track and to forecast the market prices are both important
   tasks in agri-management, by which the production schedule can be
   adjusted to increase the profit. For tracking the crop prices, the
   Council of Agriculture (COA) establishes an official website that
   provides open data of daily market prices from over 15 local markets
   with more than 100 different crops in Taiwan. Recently, the smart
   agri-management platform (S.A.M.P.) is developed by the Institute for
   Information Industry (III) as an integrated cloud service for
   agri-business. Inspired by the open data of crop prices, in this paper
   we develop a crop price forecasting service on S.A.M.P., which
   automatically retrieves the historical prices on the official website as
   training dataset, and provides the price forecasting service with some
   well-known algorithms for time series analysis. The algorithms
   implemented in this paper are the autoregressive integrated moving
   average (ARIMA), the partial least square (PLS), and the artificial
   neural network (ANN). In addition, for PLS we further integrate the
   response surface methodology (RSM), deriving a new algorithm RSMPLS, by
   which the non-linear relationship between historical prices can be
   investigated. We compare the performance of these four algorithms with
   the price data obtained from the First Fruit and Vegetable Wholesale
   Market in Taipei. The experimented crops are cabbage, bok choy,
   watermelon, and cauliflower. According to the experimental results, PLS
   and ANN are of lower error in percentages. In addition, PLS and ANN are
   recommended for short term and long term forecasting, respectively.}},
DOI = {{10.1109/TAAI.2015.7407108}},
ISBN = {{978-1-4673-9606-6}},
Unique-ID = {{ISI:000380406200019}},
}

@article{ ISI:000184443900003,
Author = {Mekle, R and Laine, AF and Wu, EX},
Title = {{Combined MR data acquisition of multicontrast images using variable
   acquisition parameters and k-space data sharing}},
Journal = {{IEEE TRANSACTIONS ON MEDICAL IMAGING}},
Year = {{2003}},
Volume = {{22}},
Number = {{7}},
Pages = {{806-823}},
Month = {{JUL}},
Abstract = {{A new technique to reduce clinical magnetic resonance imaging (MRI) scan
   time by varying acquisition parameters and sharing k-space data between
   images, is proposed. To improve data utilization, acquisition of
   multiple images of different contrast is combined into a single scan,
   with variable acquisition parameters including repetition time (TR),
   echo time (TE), and echo train length (ETL). This approach is thus
   referred to as a ``combo acquisition.{''} As a proof of concept,
   simulations of MRI experiments using spin echo (SE) and fast SE (FSE)
   sequences were performed based on Bloch equations. Predicted scan time
   reductions of 25\%-50\% were achieved for 2-contrast and 3-contrast
   combo acquisitions. Artifacts caused by nonuniform k-space data
   weighting were suppressed through semi-empirical optimization of
   parameter variation schemes and the phase encoding order. Optimization
   was assessed by minimizing three quantitative criteria: energy of the
   ``residue point spread function (PSF),{''} energy of ``residue
   profiles{''} across sharp tissue boundaries, and energy of ``residue
   images.{''} In addition, results were further evaluated by
   quantitatively analyzing the preservation of contrast, the PSF, and the
   signal-to-noise ratio. Finally, conspicuity of lesions was investigated
   for combo acquisitions in comparison with standard scans. Implications
   and challenges for the practical use of combo acquisitions are
   discussed.}},
DOI = {{10.1109/TMI.2003.815054}},
ISSN = {{0278-0062}},
ResearcherID-Numbers = {{Mekle, Ralf/S-6542-2019
   Wu, Ed Xuekui/C-1579-2009}},
ORCID-Numbers = {{Mekle, Ralf/0000-0001-6821-7722
   Wu, Ed Xuekui/0000-0001-5581-1546}},
Unique-ID = {{ISI:000184443900003}},
}

@article{ ISI:000540557900001,
Author = {Hodgin, Katie L. and von Klinggraeff, Lauren and Dauenhauer, Brian and
   McMullen, Jaimie M. and Kuhn, Ann Pulling and Stoepker, Peter and
   Carson, Russell L.},
Title = {{Effects of Sharing Data With Teachers on Student Physical Activity and
   Sedentary Behavior in the Classroom}},
Journal = {{JOURNAL OF PHYSICAL ACTIVITY \& HEALTH}},
Year = {{2020}},
Volume = {{17}},
Number = {{6}},
Pages = {{585-591}},
Month = {{JUN}},
Abstract = {{Background: Data-driven decision making is an accepted best practice in
   education, but teachers seldomreflect on data to drive their physical
   activity (PA) integration efforts. The purpose of this study was to
   explore the impact of a data-sharing intervention with classroom
   teachers on teacher-directed movement integration and students' PA and
   sedentary behavior. Methods: Teacher-directed movement behaviors from 8
   classroom teachers in 1 primary school were systematically observed
   during four 1-hour class periods before (pre) and after (post) an
   intervention in which teachers individually discussed student movement
   data with a trained interviewer. Teachers' K-2 students (N= 132) wore
   accelerometers for 10 school days both preintervention and
   postintervention. Results: Multilevel mixed effects regression indicated
   a nonsignificant increase in teacher-directed movement from
   preintervention to postintervention (+7.42\%, P =.48). Students'
   classroom time spent in moderate to vigorous PA increased (males: +2.41
   min, P <.001; females: +0.84 min, P =.04) and sedentary time decreased
   (males: -9.90 min, P <.001; females: -7.98 min, P <.001)
   postintervention. Interview data inductively analyzed revealed teachers'
   perspectives, including their surprise at low student PA during the
   school day. Conclusions: Findings suggest that sharing data with
   classroom teachers can improve student PA and decrease sedentary
   behavior at school.}},
DOI = {{10.1123/jpah.2018-0711}},
ISSN = {{1543-3080}},
EISSN = {{1543-5474}},
ResearcherID-Numbers = {{Stoepker, Peter/ABG-5501-2020}},
Unique-ID = {{ISI:000540557900001}},
}

@article{ ISI:000424592600006,
Author = {Zuiderwijk, Anneke},
Title = {{Analysing Open Data in Virtual Research Environments: New Collaboration
   Opportunities to Improve Policy Making}},
Journal = {{INTERNATIONAL JOURNAL OF ELECTRONIC GOVERNMENT RESEARCH}},
Year = {{2017}},
Volume = {{13}},
Number = {{4, SI}},
Pages = {{76-92}},
Month = {{OCT-DEC}},
Abstract = {{This article describes how virtual research environments (VREs) offer
   new opportunities for researchers to analyse open data and to obtain new
   insights for policy making. Although various VRE-related initiatives are
   under development, there is a lack of insight into how VREs support
   collaborative open data analysis by researchers and how this might be
   improved, ultimately leading to input for policy making to solve
   societal issues. This article clarifies in which ways VREs support
   researchers in open data analysis. Seven cases presenting different
   modes of researcher support for open data analysis were investigated and
   compared. Four types of support were identified: 1)'Figure it out
   yourself', 2)'Leading users by the hand', 3)'Training to provide the
   basics' and 4)'Learning from peers'. The author provides recommendations
   to improve the support of researchers' open data analysis and to
   subsequently obtain new insights for policy making to solve societal
   challenges.}},
DOI = {{10.4018/IJEGR.2017100105}},
ISSN = {{1548-3886}},
EISSN = {{1548-3894}},
ResearcherID-Numbers = {{Zuiderwijk, Anneke/AAQ-3370-2020}},
ORCID-Numbers = {{Zuiderwijk, Anneke/0000-0002-3552-7289}},
Unique-ID = {{ISI:000424592600006}},
}

@article{ ISI:000373484700004,
Author = {Gertrudis-Casado, Maria-Carmen and Gertrudix-Barrio, Manuel and
   Alvarez-Garcia, Sergio},
Title = {{Professional Information Skills and Open Data. Challenges for Citizen
   Empowerment and Social Change}},
Journal = {{COMUNICAR}},
Year = {{2016}},
Volume = {{24}},
Number = {{47}},
Pages = {{39-47}},
Month = {{APR 1}},
Abstract = {{The current process of social transformation is driven by the growth of
   the culture of transparency and accountability, the socio-technological
   development of the web and the opening of public data. This situation
   forces the media to rethink their models of social intermediation,
   converting the growing open data access and user participation into new
   instruments that facilitate citizen empowerment. Open data can only
   generate citizen empowerment, facilitate decision-making and democratic
   action if it can provide value-added information to the citizens.
   Therefore, the aim of the research is to analyse the competencies
   necessary to develop information products created with open data. The
   study used a qualitative methodology based on two instruments: a survey
   of data journalism experts (university professors of journalism,
   journalism professional data, and experts in transparency), and an
   analysis of selected cases of information products created with open
   data. The results allow the identification of a series of conceptual,
   procedural and attitudinal skills needed to perform the tasks of
   collection, processing, analysis and presentation of data, which are
   necessary for the development of this type of information product, and
   which should be integrated into the training of future journalists.}},
DOI = {{10.3916/C47-2016-04}},
ISSN = {{1134-3478}},
EISSN = {{1988-3293}},
ResearcherID-Numbers = {{Garcia, Sergio Alvarez/G-3970-2015
   del Carmen Gertrudis Casado, Maria/H-7981-2015
   Alvarez-Garcia, Sergio/N-2190-2019
   Gertrudix, Manuel/C-3604-2013}},
ORCID-Numbers = {{Garcia, Sergio Alvarez/0000-0001-9477-2148
   del Carmen Gertrudis Casado, Maria/0000-0001-6770-203X
   Alvarez-Garcia, Sergio/0000-0001-9477-2148
   Gertrudix, Manuel/0000-0002-5869-3116}},
Unique-ID = {{ISI:000373484700004}},
}

@inproceedings{ ISI:000625208502043,
Author = {Gasco, Mila and Feng, Wenhui and Gil-Garcia, J. Ramon},
Editor = {{Bui, TX}},
Title = {{Providing Public Value through Data Sharing: Understanding Critical
   Factors of Food Traceability for Local Farms and Institutional Buyers}},
Booktitle = {{PROCEEDINGS OF THE 51ST ANNUAL HAWAII INTERNATIONAL CONFERENCE ON SYSTEM
   SCIENCES (HICSS)}},
Year = {{2018}},
Pages = {{2276-2285}},
Note = {{51st Annual Hawaii International Conference on System Sciences (HICSS),
   HI, JAN 02-06, 2018}},
Organization = {{Pacific Res Inst Informat Syst \& Management; Shidler Coll Business;
   IBM; Bizgenics Fdn; Arizona Eller; AIS; Baylor Business Informat Syst;
   Int Soc Serv Innovat; St Johns Univ, Coll Profess Studies; Syracuse
   Univ, Sch Informat Stud}},
Abstract = {{Many of the datasets that could contribute to solutions for current
   public problems are proprietary and reside outside of government
   agencies. Accelerating data sharing and collaboration between those who
   hold valuable data and those able to deliver solutions is key to
   generating public value from private data. There is still a limited body
   of literature, however, that addresses data sharing and collaboration
   between private and public organizations. Using a case study of food
   traceability from local farms to institutions, this paper contributes to
   this emerging field by identifying challenges and incentives in data
   sharing among different types of organizations. In particular, our goal
   is to study how small farms and institutional buyers can be incentivized
   to share their data in a way that contributes to food safety, public
   health, and other societal goals. Our findings demonstrate that
   initiatives which can show the benefits of having a whole-chain food
   traceability system, have clear policies and regulations, and
   opportunities for participation in training activities are key
   incentives.}},
ISBN = {{978-0-9981331-1-9}},
Unique-ID = {{ISI:000625208502043}},
}

@article{ ISI:000549417000001,
Author = {Vince, Nicolas and Douillard, Venceslas and Geffard, Estelle and Meyer,
   Diogo and Castelli, Erick C. and Mack, Steven J. and Limou, Sophie and
   Gourraud, Pierre-Antoine},
Title = {{SNP-HLA Reference Consortium (SHLARC): HLA and SNP data sharing for
   promoting MHC-centric analyses in genomics}},
Journal = {{GENETIC EPIDEMIOLOGY}},
Year = {{2020}},
Volume = {{44}},
Number = {{7}},
Pages = {{733-740}},
Month = {{OCT}},
Abstract = {{Genome-wide associations studies have repeatedly identified the major
   histocompatibility complex genomic region (6p21.3) as key in immune
   pathologies. Researchers have also aimed to extend the biological
   interpretation of associations by focusing directly on human leukocyte
   antigen (HLA) polymorphisms and their combination as haplotypes. To
   circumvent the effort and high costs of HLA typing, statistical
   solutions have been developed to inferHLAalleles from single-nucleotide
   polymorphism (SNP) genotyping data. ThoughHLAimputation methods have
   been developed, no unified effort has yet been undertaken to share large
   and diverse imputation models, or to improve methods. By training the
   HIBAG software on SNP + HLA data generated by the Consortium on Asthma
   among African-ancestry Populations in the Americas (CAAPA) to create
   reference panels, we highlighted the importance of (a) the number of
   individuals in reference panels, with a twofold increase in accuracy
   (from 10 to 100 individuals) and (b) the number of SNPs, with a 1.5-fold
   increase in accuracy (from 500 to 24,504 SNPs). Results showed improved
   accuracy with CAAPA compared to the African American models available in
   HIBAG, highlighting the need for precise population-matching. The
   SNP-HLA Reference Consortium is an international endeavor to gather
   data, enhanceHLAimputation and broaden access to highly accurate
   imputation models for the immunogenomics community.}},
DOI = {{10.1002/gepi.22334}},
Early Access Date = {{JUL 2020}},
ISSN = {{0741-0395}},
EISSN = {{1098-2272}},
ResearcherID-Numbers = {{Vince, Nicolas/K-5882-2019
   }},
ORCID-Numbers = {{Douillard, Venceslas/0000-0002-6762-4083}},
Unique-ID = {{ISI:000549417000001}},
}

@article{ ISI:000426734600116,
Author = {Wang, Seunghyeon and Hae, Hyeonyong and Kim, Juhyung},
Title = {{Development of Easily Accessible Electricity Consumption Model Using
   Open Data and GA-SVR}},
Journal = {{ENERGIES}},
Year = {{2018}},
Volume = {{11}},
Number = {{2}},
Month = {{FEB}},
Abstract = {{In many countries, DR (Demand Response) has been developed for which
   customers are motivated to save electricity by themselves during peak
   time to prevent grand-scale blackouts. One of the common methods in DR,
   is CPP (Critical Peak Pricing). Predicting energy consumption is
   recognized as one of the tool for dealing with CPP. There are a variety
   of studies in developing the model of energy consumption, which is based
   on energy simulation, data-driven model or metamodelling. However, it is
   difficult for general users to use these models due to requirement of
   various sensing data and expertise. And it also takes long time to
   simulate the models. These limitations can be an obstacle for achieving
   CPP's purpose that encourages general users to manage their energy usage
   by themselves. As an alternative, this research suggests to use open
   data and GA (Genetic Algorithm)-SVR (Support Vector Regression). The
   model is applied to a hospital in Korea and 34,636 data sets (1 year)
   are collected while 31,756 (11 months) sets are used for training and
   2880 sets (1 month) are used for validation. As a result, the
   performance of proposed model is 14.17\% in CV (RMSE), which satisfies
   the Korea Energy Agency's and ASHRAE (American Society of Heating,
   Refrigerating and Air-Conditioning Engineers) error allowance range of
   +/- 30\%, and +/- 20\% respectively.}},
DOI = {{10.3390/en11020373}},
Article-Number = {{373}},
EISSN = {{1996-1073}},
ORCID-Numbers = {{Wang, Seunghyeon/0000-0003-4189-1789}},
Unique-ID = {{ISI:000426734600116}},
}

@inproceedings{ ISI:000494420400019,
Author = {Aulia, Givo and Kurniawan, Teguh},
Book-Group-Author = {{IEEE}},
Title = {{The Implementation of Open Data Program in the Special Capital Region
   (DKI) of Jakarta Province}},
Booktitle = {{PROCEEDINGS OF 2018 3RD INTERNATIONAL CONFERENCE ON SUSTAINABLE
   INFORMATION ENGINEERING AND TECHNOLOGY (SIET 2018)}},
Year = {{2018}},
Pages = {{96-100}},
Note = {{3rd International Conference on Sustainable Information Engineering and
   Technology (SIET), Univ Brawijaya, Fac Comp Sci, Malang, INDONESIA, NOV
   10-12, 2018}},
Organization = {{IEEE Indonesia Sect}},
Abstract = {{The research discusses the implementation of open data program in DKI
   Jakarta Province. DKI Jakarta is the first region to implement an open
   data program in Indonesia. The implementation of open data program
   should pay attention to the various supporting aspects to run properly.
   This research uses a post-positivist approach by qualitative data
   analysis technique. The theory applied to this research is a theory of
   implementation drivers. There are three core components in the
   implementation drivers, namely competency drivers, organization drivers,
   and leadership drivers. The result of the research shows that the
   implementation of open data program has some disadvantages in fulfilling
   implementation drivers components for competency drivers and
   organization drivers. In the competency drivers component, indicators
   that have been in trouble are in staff selection, training, and
   coaching. Furthermore, in the organization drivers component, the
   problematic indicators are in facilitative administration and system
   level intervention.}},
ISBN = {{978-1-5386-7407-9}},
ResearcherID-Numbers = {{Kurniawan, Teguh/C-4166-2015}},
ORCID-Numbers = {{Kurniawan, Teguh/0000-0002-0736-0258}},
Unique-ID = {{ISI:000494420400019}},
}

@article{ ISI:000460600800001,
Author = {Tian, Yu and Shang, Yong and Tong, Dan-Yang and Chi, Sheng-Qiang and Li,
   Jun and Kong, Xiang-Xing and Ding, Ke-Feng and Li, Jing-Song},
Title = {{POPCORN: A web service for individual PrognOsis prediction based on
   multi-center clinical data CollabORatioN without patient-level data
   sharing}},
Journal = {{JOURNAL OF BIOMEDICAL INFORMATICS}},
Year = {{2018}},
Volume = {{86}},
Pages = {{1-14}},
Month = {{OCT}},
Abstract = {{Background and objective: Clinical prognosis prediction plays an
   important role in clinical research and practice. The construction of
   prediction models based on electronic health record data has recently
   become a research focus. Due to the lack of external validation,
   prediction models based on single-center, hospital-specific datasets may
   not perform well with datasets from other medical institutions.
   Therefore, research investigating prognosis prediction model
   construction based on a collaborative analysis of multi-center
   electronic health record data could increase the number and coverage of
   patients used for model training, enrich patient prognostic features and
   ultimately improve the accuracy and generalization of prognosis
   prediction.
   Materials and methods: A web service for individual prognosis prediction
   based on multi-center clinical data collaboration without patient-level
   data sharing (POPCORN) was proposed. POPCORN focuses on solving key
   issues in multi-center collaborative research based on electronic health
   record systems; these issues include the standardization of clinical
   data expression, the preservation of patient privacy during model
   training and the effect of case mix variance on the prediction model
   construction and application. POPCORN is based on a multivariable
   meta-analysis and a Bayesian framework and can construct suitable
   prediction models for multiple clinical scenarios that can effectively
   adapt to complex clinical application environments.
   Results: POPCORN was validated using a joint, multi-center collaborative
   research network between China and the United States with patients
   diagnosed with colorectal cancer. The performance of the models based on
   POPCORN was comparable-to that-of-the standard prognosis prediction
   model; however, POPCORN did not expose raw patient data. The prediction
   models had similar AUC, but the BMA model had the lowest ECI across all
   prediction models, indicating that this model had better calibration
   performance than the other models, especially for patients in Chinese
   hospitals.
   Conclusions: The POPCORN system can build prediction models that perform
   well in complex clinical application scenarios and can provide effective
   decision support for individual patient prognostic predictions.}},
DOI = {{10.1016/j.jbi.2018.08.008}},
ISSN = {{1532-0464}},
EISSN = {{1532-0480}},
Unique-ID = {{ISI:000460600800001}},
}

@article{ ISI:000571115700001,
Author = {Manuel Barrera, Jose and Reina, Alejandro and Mate, Alejandro and Carlos
   Trujillo, Juan},
Title = {{Solar Energy Prediction Model Based on Artificial Neural Networks and
   Open Data}},
Journal = {{SUSTAINABILITY}},
Year = {{2020}},
Volume = {{12}},
Number = {{17}},
Month = {{SEP}},
Abstract = {{With climate change driving an increasingly stronger influence over
   governments and municipalities, sustainable development, and renewable
   energy are gaining traction across the globe. This is reflected within
   the EU 2030 agenda, that envisions a future where there is universal
   access to affordable, reliable and sustainable energy. One of the
   challenges to achieve this vision lies on the low reliability of certain
   renewable sources. While both particulars and public entities try to
   reach self-sufficiency through sustainable energy generation, it is
   unclear how much investment is needed to mitigate the unreliability
   introduced by natural factors such as varying wind speed and daylight
   across the year. In this sense, a tool that aids predicting the energy
   output of sustainable sources across the year for a particular location
   can aid greatly in making sustainable energy investments more efficient.
   In this paper, we make use of Open Data sources, Internet of Things
   (IoT) sensors and installations distributed across Europe to create such
   tool through the application of Artificial Neural Networks. We analyze
   how the different factors affect the prediction of energy production and
   how Open Data can be used to predict the expected output of sustainable
   sources. As a result, we facilitate users the necessary information to
   decide how much they wish to invest according to the desired energy
   output for their particular location. Compared to state-of-the-art
   proposals, our solution provides an abstraction layer focused on energy
   production, rather that radiation data, and can be trained and tailored
   for different locations using Open Data. Finally, our tests show that
   our proposal improves the accuracy of the forecasting, obtaining a lower
   mean squared error (MSE) of 0.040 compared to an MSE 0.055 from other
   proposals in the literature.}},
DOI = {{10.3390/su12176915}},
Article-Number = {{6915}},
EISSN = {{2071-1050}},
ResearcherID-Numbers = {{Mate, Alejandro/AAG-7809-2021
   }},
ORCID-Numbers = {{Mate, Alejandro/0000-0001-7770-3693
   Trujillo, Juan/0000-0003-0139-6724
   Arroyo, Jose/0000-0002-3359-2685
   Reina Reina, Alejandro/0000-0001-9949-2735}},
Unique-ID = {{ISI:000571115700001}},
}

@article{ ISI:000493118300009,
Author = {Arboleda Mazo, Walter Hugo and Anaya Hernandez, Raquel},
Title = {{An Approach to Open Data in Health and its Current Status in Colombia}},
Journal = {{REVISTA PENSAMIENTO AMERICANO}},
Year = {{2018}},
Volume = {{11}},
Number = {{21}},
Pages = {{110-126}},
Month = {{JUL-DEC}},
Abstract = {{The use of open data is a strategy that promotes transparency and
   visibility of the government entities management in order to providing
   timely and reliable data to the different sectors for informed decision
   making. Countries like the United States and the United Kingdom have
   defined frameworks that define a unified strategy for open data on
   health, called Open Health Data (OHD); also the World Health
   Organization (WHO) proposes a network called Health Metrics Network
   (HMN), to support countries in their purpose of improving global health
   through the strengthening of health information systems. In Colombia,
   the open data strategy which aims to support online government
   initiatives, has been strengthening and maturing. However, there are
   still ongoing challenges that must be faced in strengthening health open
   data, such as: culture change of the organisms responsible for the data
   to share, where the key factor is citizenship training; territorial
   entities empowerment that allows them to understand and visualize this
   approach's potential, guidelines implementation on the individual data
   anonymization to satisfy the regulations in terms of information privacy
   and the stimulation of the use of open data by ICT professionals for new
   products and services creation, guaranteeing a true government open data
   impact in the country. A proposal is made for an operational model that
   illustrates the way in which all actors and information systems can be
   articulated in a health information systems ecosystem that is integrated
   with the open data catalog.}},
DOI = {{10.21803/pensam.v11i21.155}},
ISSN = {{2027-2448}},
Unique-ID = {{ISI:000493118300009}},
}

@article{ ISI:000500387400009,
Author = {Fan, Brandon and Fan, Weiguo and Smith, Carly and Garner, Harold Skip},
Title = {{Adverse drug event detection and extraction from open data: A deep
   learning approach}},
Journal = {{INFORMATION PROCESSING \& MANAGEMENT}},
Year = {{2020}},
Volume = {{57}},
Number = {{1}},
Month = {{JAN}},
Abstract = {{Drug prescription is a task that doctors face daily with each patient.
   However, when prescribing drugs, doctors must be conscious of all
   potential drug side effects. In fact, according to the U.S. Department
   of Health and Human Services, adverse drug events (ADEs), or harmful
   side effects, account for 1/3 of total hospital admissions each year.
   The goal of this research is to utilize novel deep learning methods for
   accurate detection and identification of professionally unreported drug
   side effects using widely available public data (open data). Utilizing a
   manually-labelled dataset of 10,000 reviews gathered from WebMD and
   Drugs.com , this research proposes a deep learning-based approach
   utilizing Bidirectional Encoder Representations from Transformers (BERT)
   based models for ADE detection and extraction and compares results to
   standard deep learning models and current state-of-the-art extraction
   models. By utilizing a hybrid of transfer learning from pre-trained BERT
   representations and sentence embeddings, the proposed model achieves an
   AUC score of 0.94 for ADE detection and an F1 score of 0.97 for ADE
   extraction. Previous state of the art deep learning approach achieves an
   AUC of 0.85 in ADE detection and an F1 of 0.82 in ADE extraction on our
   dataset of review texts. The results show that a BERT-based model
   achieves new state-of-the-art results on both the ADE detection and
   extraction task. This approach can be applied to multiple healthcare and
   information extraction tasks and used to help solve the problem that
   doctors face when prescribing drugs. Overall, this research introduces a
   novel dataset utilizing social media health forum data and shows the
   viability and capability of using deep learning techniques in ADE
   detection and extraction as well as information extraction as a whole.
   The model proposed in this paper achieves state-of-the-art results and
   can be applied to multiple other healthcare and information extraction
   tasks including medical entity extraction and entity recognition.}},
DOI = {{10.1016/j.ipm.2019.102131}},
Article-Number = {{102131}},
ISSN = {{0306-4573}},
EISSN = {{1873-5371}},
ResearcherID-Numbers = {{Fan, Weiguo/E-6343-2012}},
ORCID-Numbers = {{Fan, Weiguo/0000-0003-1272-5538}},
Unique-ID = {{ISI:000500387400009}},
}

@article{ ISI:000581589300010,
Author = {Ifeanyi-obi, C. C. and Ibiso, H. D.},
Title = {{Extension Agents Perception of Open Data Usage in Agricultural
   Communication in Abia State}},
Journal = {{JOURNAL OF AGRICULTURAL EXTENSION}},
Year = {{2020}},
Volume = {{24}},
Number = {{4}},
Pages = {{91-99}},
Month = {{OCT}},
Abstract = {{The study assessed agricultural extension agent's perception of open
   data usage in agricultural communication in Abia State. Simple random
   sampling technique was used to select 83 extension agents for the study.
   Data for the study was collected with the use of questionnaire and
   described with frequency counts, percentages and mean. A moderate
   percentage (68\%) of extension agents were aware of open data with only
   15\% and 7\% having high and very high level of awareness. A relatively
   high percentage (54\%) understand what open data is all about with only
   49\% consenting to be involved in the use of open data mainly at a
   moderate level (66\%). Extension agents mainly perceive open data to
   facilitate agricultural research ((x) over bar = 3.2), facilitate access
   to research findings ((x) over bar = 3.2), give access to the work of
   other extension agents in other states ((x) over bar = 3.2), expose them
   to innovation in agriculture ((x) over bar = 3.2) and ensure timely
   access to all necessary information ((x) over bar = 3.1). The major
   challenges to the use of open data among extension agents were lack of
   fund to purchase internet bundle ((x) over bar = 2.2), unavailability of
   internet facilities((x) over bar = 2.1), lack of basic ICT facilities
   ((x) over bar = 2.1) as well as lack of adequate knowledge of the use of
   ICT((x) over bar = 2.0) and open data ((x) over bar = 2.0). Propagation
   of the use of open data could be done mainly by provision of ICTs in
   offices ((x) over bar = 3.6), training of extension agents to understand
   the concept of open data ((x) over bar = 3.6) and provision of internet
   facilities for extension agents ((x) over bar = 3.5). The study
   emphasized the need for Government through the Agricultural Development
   Programme (ADP) to provide basic ICT tools in offices and provision of
   internet facilities to enable extension agents maximize the use of open
   data.}},
DOI = {{10.4314/jae.v24i4.10}},
ISSN = {{1119-944X}},
EISSN = {{2408-6851}},
Unique-ID = {{ISI:000581589300010}},
}

@article{ ISI:000454535600062,
Author = {Lorenzo, Armando J. and Rickard, Mandy and Braga, Luis H. and Guo, Yanbo
   and Oliveria, John-Paul},
Title = {{Predictive Analytics and Modeling Employing Machine Learning Technology:
   The Next Step in Data Sharing, Analysis, and Individualized Counseling
   Explored With a Large, Prospective Prenatal Hydronephrosis Database}},
Journal = {{UROLOGY}},
Year = {{2019}},
Volume = {{123}},
Pages = {{204-208}},
Month = {{JAN}},
Abstract = {{OBJECTIVE To explore the potential value of utilizing a commercially
   available cloud-based machine learning platform to predict surgical
   intervention in infants with prenatal hydronephrosis (HN).
   MATERIALS AND METHODS A prospective prenatal HN database was uploaded
   into Microsoft Azure Machine Learning Studio. Probabilistic principal
   component analysis was employed for data imputation. Multiple clinical
   variables were included in two-class decision jungle and neural network
   for model training, using surgical intervention as the primary outcome.
   Models were scored and evaluated after a 70/30 split of the data.
   RESULTS A total of 557 entries were included. The optimized model
   (decision jungle) achieved an area under the curve of 0.9, accuracy of
   0.87, and precision of 0.80, employing a threshold of 0.5 to predict
   surgery. Average time to train, score and evaluate the model was 5
   seconds. The predictive model was deployed as a web service in 35
   seconds, generating a unique API key for app and webpage development.
   Individualized prediction based on the included variables was deployed
   as a web-based and batch execution Excel file in less than one minute.
   CONCLUSION This cloud-based ML technology allows easy building,
   deployment, and sharing of predictive analytics solutions. Using
   prenatal HN as an example, we propose an opportunity to address
   contemporary challenges with data analysis, reporting a creative
   solution that moves beyond the current standard. (C) 2018 Elsevier Inc.}},
DOI = {{10.1016/j.urology.2018.05.041}},
ISSN = {{0090-4295}},
EISSN = {{1527-9995}},
ResearcherID-Numbers = {{Oliveria, John-Paul/AAH-8080-2020
   Guo, Yanbo/ABI-3574-2020}},
ORCID-Numbers = {{Oliveria, John-Paul/0000-0002-7192-7634
   }},
Unique-ID = {{ISI:000454535600062}},
}

@article{ ISI:000380121200001,
Author = {Niu, Ai-qin and Xie, Liang-jun and Wang, Hui and Zhu, Bing and Wang,
   Sheng-qi},
Title = {{Prediction of selective estrogen receptor beta agonist using open data
   and machine learning approach}},
Journal = {{DRUG DESIGN DEVELOPMENT AND THERAPY}},
Year = {{2016}},
Volume = {{10}},
Pages = {{2323-2331}},
Abstract = {{Background: Estrogen receptors (ERs) are nuclear transcription factors
   that are involved in the regulation of many complex physiological
   processes in humans. ERs have been validated as important drug targets
   for the treatment of various diseases, including breast cancer, ovarian
   cancer, osteoporosis, and cardiovascular disease. ERs have two subtypes,
   ER-alpha and ER-beta. Emerging data suggest that the development of
   subtype-selective ligands that specifically target ER-beta could be a
   more optimal approach to elicit beneficial estrogen-like activities and
   reduce side effects.
   Methods: Herein, we focused on ER-beta and developed its in silico
   quantitative structure-activity relationship models using machine
   learning (ML) methods.
   Results: The chemical structures and ER-beta bioactivity data were
   extracted from public chemo-genomics databases. Four types of popular
   fingerprint generation methods including MACCS fingerprint, PubChem
   fingerprint, 2D atom pairs, and Chemistry Development Kit extended
   fingerprint were used as descriptors. Four ML methods including Naive
   Bayesian classifier, k-nearest neighbor, random forest, and support
   vector machine were used to train the models. The range of
   classification accuracies was 77.10\% to 88.34\%, and the range of area
   under the ROC (receiver operating characteristic) curve values was
   0.8151 to 0.9475, evaluated by the 5-fold cross-validation. Comparison
   analysis suggests that both the random forest and the support vector
   machine are superior for the classification of selective ER-beta
   agonists. Chemistry Development Kit extended fingerprints and MACCS
   fingerprint performed better in structural representation between active
   and inactive agonists.
   Conclusion: These results demonstrate that combining the fingerprint and
   ML approaches leads to robust ER-beta agonist prediction models, which
   are potentially applicable to the identification of selective ER-beta
   agonists.}},
DOI = {{10.2147/DDDT.S110603}},
ISSN = {{1177-8881}},
Unique-ID = {{ISI:000380121200001}},
}

@article{ ISI:000598123400002,
Author = {Bizzego, Andrea and Lim, Mengyu and Schiavon, Greta and Setoh, Peipei
   and Gabrieli, Giulio and Dimitriou, Dagmara and Esposito, Gianluca},
Title = {{Child disability and caregiving in low and middle income countries: Big
   data approach on open data}},
Journal = {{RESEARCH IN DEVELOPMENTAL DISABILITIES}},
Year = {{2020}},
Volume = {{107}},
Month = {{DEC}},
Abstract = {{Background: The presence of child disabilities might affect the amount
   of caregiving attention the child receives, with potential ramifications
   on the development of the child and increasing the likelihood of
   developing a more severe condition. Little is known about the
   association between child disabilities and caregiving practices in less
   developed countries, penalized by both lack of data and a research bias
   toward western societies.
   Method: In this study, we apply data mining methods on a large (N =
   29,525) dataset from UNICEF to investigate the association between
   caregiving practices and developmental disabilities of the children, and
   highlight the differences between intellectual and other disabilities.
   Results: Our results highlight that, compared to other types of
   disabilities, intellectual disabilities increased the risk of being
   neglected by the caregiver in those activities oriented to the cognitive
   development. The education of the caregiver and the socioeconomical
   development of the country are actively involved in the moderation of
   the risk.
   Conclusion: We demonstrated that educational policies of parental
   training, such as psycho education regarding intellectual disabilities
   and destigmatization campaigns, are needed to benefit parental practices
   in lowand middle-income countries.}},
DOI = {{10.1016/j.ridd.2020.103795}},
Article-Number = {{103795}},
ISSN = {{0891-4222}},
ORCID-Numbers = {{Gabrieli, Giulio/0000-0002-9846-5767}},
Unique-ID = {{ISI:000598123400002}},
}

@article{ ISI:000574971500001,
Author = {Hersh, Jonathan and Engstrom, Ryan and Mann, Michael},
Title = {{Open data for algorithms: mapping poverty in Belize using open satellite
   derived features and machine learning}},
Journal = {{INFORMATION TECHNOLOGY FOR DEVELOPMENT}},
Year = {{2021}},
Volume = {{27}},
Number = {{2}},
Pages = {{263-292}},
Month = {{APR 3}},
Abstract = {{Mapping the spatial distribution of poverty and incomes within a country
   remains a challenge. Recently, several proposed methods incorporate
   features from satellite imagery to improve model performance (Babenko et
   al.,2017, Poverty mapping using convolutional neural networks trained on
   high and medium resolution satellite images, with an application in
   Mexico. ArXiv Preprint ArXiv:1711.06323) or supplant small area
   estimation methods (Jean et al.,2016, Combining satellite imagery and
   machine learning to predict poverty. Science, 353(6301), 790-94.
   doi:10.1126/science.aaf7894; Engstrom et al.,2017, Poverty from space:
   Using high-resolution satellite imagery for estimating economic
   well-being.). However, these methods require high-spatial resolution
   imagery which, given their cost and infrequent acquisition, may render
   these advances impractical for most applications. We investigate how
   small area estimates of average income may improve when incorporating
   features derived from Sentinel-2 and MODIS imagery. Both satellites
   provide free imagery, have global coverage, and a frequent revisit rate.
   We estimate a poverty map for Belize which incorporates spatial and time
   series features derived from these sensors, with and without survey
   derived variables. We document an 8\% percent improvement in model
   performance when including these satellite features. We conclude by
   arguing that Open Data for Development should include open data
   pipelines where possible.}},
DOI = {{10.1080/02681102.2020.1811945}},
Early Access Date = {{OCT 2020}},
ISSN = {{0268-1102}},
EISSN = {{1554-0170}},
ORCID-Numbers = {{Hersh, Jonathan/0000-0001-6786-5162
   Mann, Michael/0000-0002-6268-6867}},
Unique-ID = {{ISI:000574971500001}},
}

@article{ ISI:000334176700001,
Author = {Volk, Carol J. and Lucero, Yasmin and Barnas, Katie},
Title = {{Why is Data Sharing in Collaborative Natural Resource Efforts so Hard
   and What can We Do to Improve it?}},
Journal = {{ENVIRONMENTAL MANAGEMENT}},
Year = {{2014}},
Volume = {{53}},
Number = {{5}},
Pages = {{883-893}},
Month = {{MAY}},
Abstract = {{Increasingly, research and management in natural resource science rely
   on very large datasets compiled from multiple sources. While it is
   generally good to have more data, utilizing large, complex datasets has
   introduced challenges in data sharing, especially for collaborating
   researchers in disparate locations ({''}distributed research teams{''}).
   We surveyed natural resource scientists about common data-sharing
   problems. The major issues identified by our survey respondents (n =
   118) when providing data were lack of clarity in the data request
   (including format of data requested). When receiving data, survey
   respondents reported various insufficiencies in documentation describing
   the data (e.g., no data collection description/no protocol, data
   aggregated, or summarized without explanation). Since metadata, or
   ``information about the data,{''} is a central obstacle in efficient
   data handling, we suggest documenting metadata through data
   dictionaries, protocols, read-me files, explicit null value
   documentation, and process metadata as essential to any large-scale
   research program. We advocate for all researchers, but especially those
   involved in distributed teams to alleviate these problems with the use
   of several readily available communication strategies including the use
   of organizational charts to define roles, data flow diagrams to outline
   procedures and timelines, and data update cycles to guide data-handling
   expectations. In particular, we argue that distributed research teams
   magnify data-sharing challenges making data management training even
   more crucial for natural resource scientists. If natural resource
   scientists fail to overcome communication and metadata documentation
   issues, then negative data-sharing experiences will likely continue to
   undermine the success of many large-scale collaborative projects.}},
DOI = {{10.1007/s00267-014-0258-2}},
ISSN = {{0364-152X}},
EISSN = {{1432-1009}},
Unique-ID = {{ISI:000334176700001}},
}

@article{ ISI:000431427400001,
Author = {Yokoya, Naoto and Ghamisi, Pedram and Xia, Junshi and Sukhanov, Sergey
   and Heremans, Roel and Tankoyeu, Ivan and Bechtel, Benjamin and Le Saux,
   Bertrand and Moser, Gabriele and Tuia, Devis},
Title = {{Open Data for Global Multimodal Land Use Classification: Outcome of the
   2017 IEEE GRSS Data Fusion Contest}},
Journal = {{IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE
   SENSING}},
Year = {{2018}},
Volume = {{11}},
Number = {{5}},
Pages = {{1363-1377}},
Month = {{MAY}},
Abstract = {{In this paper, we present the scientific outcomes of the 2017 Data
   Fusion Contest organized by the Image Analysis and Data Fusion Technical
   Committee of the IEEE Geoscience and Remote Sensing Society. The 2017
   Contest was aimed at addressing the problem of local climate zones
   classification based on a multi-temporal and multimodal dataset,
   including image (Landsat 8 and Sentinel-2) and vector data (from
   OpenStreetMap). The competition, based on separate geographical
   locations for the training and testing of the proposed solution, aimed
   at models that were accurate (assessed by accuracy metrics on an
   undisclosed reference for the test cities), general (assessed by
   spreading the test cities across the globe), and computationally
   feasible (assessed by having a test phase of limited time). The
   techniques proposed by the participants to the Contest spanned across a
   rather broad range of topics, and of mixed ideas and methodologies
   deriving from computer vision and machine learning but also deeply
   rooted in the specificities of remote sensing. In particular, rigorous
   atmospheric correction, the use of multidate images, and the use of
   ensemble methods fusing results obtained from different data
   sources/time instants made the difference.}},
DOI = {{10.1109/JSTARS.2018.2799698}},
ISSN = {{1939-1404}},
EISSN = {{2151-1535}},
ResearcherID-Numbers = {{ARSLAN, Okan/AAA-3232-2020
   Tuia, Devis/AAE-9339-2019
   Bechtel, Benjamin/H-9853-2014
   Le Saux, Bertrand/AAA-2534-2020
   }},
ORCID-Numbers = {{Tuia, Devis/0000-0003-0374-2459
   Bechtel, Benjamin/0000-0001-8802-7934
   Moser, Gabriele/0000-0002-3796-2938
   Le Saux, Bertrand/0000-0001-7162-6746}},
Unique-ID = {{ISI:000431427400001}},
}

@article{ ISI:000308522900004,
Author = {Enke, Neela and Thessen, Anne and Bach, Kerstin and Bendix, Joerg and
   Seeger, Bernhard and Gemeinholzer, Birgit},
Title = {{The user's view on biodiversity data sharing - Investigating facts of
   acceptance and requirements to realize a sustainable use of research
   data}},
Journal = {{ECOLOGICAL INFORMATICS}},
Year = {{2012}},
Volume = {{11}},
Number = {{SI}},
Pages = {{25-33}},
Month = {{SEP}},
Abstract = {{Data sharing has become an important issue in modern biodiversity
   research to address large scale questions. Despite the steadily growing
   scientific demand, data are not easily accessed. Why is this the case?
   This study explores the reasons for the reluctance to share data on the
   one hand and the motivations for sharing on the other by summarising
   results from >60 interviews and >700 survey participants within the
   biodiversity science community. As result, there is a clear commitment
   to share biodiversity data, but also a reluctance to actually do so due
   to a mixture of social and technical impediments, such as loss of
   control over data and lack of professional reward for sharing. This
   exploratory study summarises the formal and technical requirements for
   data sharing and reuse, stated by voluntarily participating scientists
   worldwide. To ensure sustainable data use, user friendly data
   infrastructure have to be expanded or newly designed, data management
   plans for all scientific investigations have to be promoted, training
   for the users has to be provided and motivational aspects at all stages
   of data submission and re-use have to be considered. (C) 2012 Elsevier
   BM. All rights reserved.}},
DOI = {{10.1016/j.ecoinf.2012.03.004}},
ISSN = {{1574-9541}},
EISSN = {{1878-0512}},
ResearcherID-Numbers = {{Thessen, Anne/J-6449-2012
   Gemeinholzer, Birgit/AAF-6401-2020
   }},
ORCID-Numbers = {{Thessen, Anne/0000-0002-2908-3327
   gemeinholzer, birgit/0000-0002-9145-9284}},
Unique-ID = {{ISI:000308522900004}},
}

@article{ ISI:000453153700002,
Author = {Chakraborty, Arnab and Wilson, Bev and Sarraf, Saket and Jana, Arnab},
Title = {{Open data for informal settlements: Toward a user's guide for urban
   managers and planners}},
Journal = {{JOURNAL OF URBAN MANAGEMENT}},
Year = {{2015}},
Volume = {{4}},
Number = {{2}},
Pages = {{74-91}},
Month = {{DEC}},
Abstract = {{Informal settlements exist in a legally contested space and the quality
   of - and access to - information about them has been historically
   limited. The open data movement promises to address this gap by offering
   alternative sources for information and free or low cost analytical
   platforms. However, in order to use open data effectively, urban
   managers and planners need guidance to navigate these new data sources,
   software, and server platforms, as well as acquire the necessary skills.
   In this paper, we begin to address these issues by developing a
   framework that organizes the sprawling and rapidly evolving world of
   open urban data. Our framework includes three broad categories (1)
   inputs and resources, (2) activities and outputs, and (3) outcomes. We
   then identify and describe the key subcomponents under each, and list
   the prominent products and resources available to urban managers and
   planners. For example, under inputs and resources, we discuss open urban
   data sources such as Open Street Maps, cyberinfrastructure for web
   hosting, application deployment, and data processing, and open source
   software that can be used to analyze and visualize collected or derived
   data. We also identify the key resources available to planners for
   training and discuss the complementary opportunities presented by
   conventional datasets such as census and open urban data. Finally, using
   examples from ongoing activities in Mumbai, we show how open data
   resources can be useful for understanding urbanization and better
   integrating informal settlements into formal urban management and
   planning processes. We suggest that urban managers and planners working
   in informal settlements should take greater advantage of open data
   resources in order to both better address current challenges as well as
   for shaping a better future for the communities they serve. (C) 2015
   TheAuthors. Production and Hosting by Elsevier B.V. on behalf of
   Zhejiang University and Chinese Association of Urban Management.}},
DOI = {{10.1016/j.jum.2015.12.001}},
ISSN = {{2226-5856}},
EISSN = {{2589-0360}},
ResearcherID-Numbers = {{Jana, Arnab/C-4116-2016
   }},
ORCID-Numbers = {{Jana, Arnab/0000-0001-8210-1566
   Chakraborty, Arnab/0000-0002-2899-7864
   Wilson, Bev/0000-0003-3892-456X}},
Unique-ID = {{ISI:000453153700002}},
}

@article{ ISI:000374186100072,
Author = {Lee, Jae Eun and Sung, Jung Hye and Barnett, M. Edwina and Norris, Keith},
Title = {{User-Friendly Data-Sharing Practices for Fostering Collaboration within
   a Research Network: Roles of a Vanguard Center for a Community-Based
   Study}},
Journal = {{INTERNATIONAL JOURNAL OF ENVIRONMENTAL RESEARCH AND PUBLIC HEALTH}},
Year = {{2016}},
Volume = {{13}},
Number = {{1}},
Month = {{JAN}},
Abstract = {{Although various attempts have been made to build collaborative cultures
   for data sharing, their effectiveness is still questionable. The Jackson
   Heart Study (JHS) Vanguard Center (JHSVC) at the NIH-funded Research
   Centers in Minority Institutions (RCMI) Translational Research Network
   (RTRN) Data Coordinating Center (DCC) may be a new concept in that the
   data are being shared with a research network where a plethora of
   scientists/researchers are working together to achieve their common
   goal. This study describes the current practices to share the JHS data
   through the mechanism of JHSVC. The JHS is the largest single-site
   cohort study to prospectively investigate the determinants of
   cardiovascular disease among African-Americans. It has adopted a formal
   screened access method through a formalized JHSVC mechanism, in which
   only a qualified scientist(s) can access the data. The role of the DCC
   was to help RTRN researchers explore hypothesis-driven ideas to enhance
   the output and impact of JHS data through customized services, such as
   feasibility tests, data querying, manuscript proposal development and
   data analyses for publication. DCC has implemented these various
   programs to facilitate data utility. A total of 300 investigators
   attended workshops and/ or received training booklets. DCC provided two
   online and five onsite workshops and developed/distributed more than 250
   copies of the booklet to help potential data users understand the
   structure of and access to the data. Information on data use was also
   provided through the RTRN website. The DCC efforts led to the production
   of five active manuscript proposals, seven completed publications, 11
   presentations and four NIH grant proposals. These outcomes resulted from
   activities during the first four years; over the last couple of years,
   there were few new requests. Our study suggested that DCC-customized
   services enhanced the accessibility of JHS data and their utility by
   RTRN researchers and helped to achieve the principal goal of JHSVC of
   scientific productivity. In order to achieve long-term success, the
   following, but not limited to these, should be addressed in the current
   data sharing practices: preparation of new promotional strategies in
   response to changes in technology and users' needs, collaboration with
   the Network statisticians, harmonization of the JHS data with the other
   local-based heart datasets to meet the needs of the potential users from
   the broader geographical areas, adoption of the RTRN comprehensive
   data-sharing policy to broaden the variety of research topics and
   implementation of an ongoing monitoring program to evaluate its success.}},
DOI = {{10.3390/ijerph13010034}},
EISSN = {{1660-4601}},
Unique-ID = {{ISI:000374186100072}},
}

@article{ ISI:000328892500001,
Author = {Vanneste, Dirk and Vermeulen, Bram and Declercq, Anja},
Title = {{Healthcare professionals' acceptance of BelRAI, a web-based system
   enabling person-centred recording and data sharing across care settings
   with interRAI instruments: a UTAUT analysis}},
Journal = {{BMC MEDICAL INFORMATICS AND DECISION MAKING}},
Year = {{2013}},
Volume = {{13}},
Month = {{NOV 27}},
Abstract = {{Background: Healthcare and social care environments are increasingly
   confronted with older persons with long-term care needs. Consequently,
   the need for integrated and coordinated assessment systems increases. In
   Belgium, feasibility studies have been conducted on the implementation
   and use of interRAI instruments offering opportunities to improve
   continuity and quality of care. However, the development and
   implementation of information technology to support a shared dataset is
   a difficult and gradual process. We explore the applicability of the
   UTAUT theoretical model in the BelRAI healthcare project to analyse the
   acceptance of the BelRAI web application by healthcare professionals in
   home care, nursing home care and acute hospital care for older people
   with disabilities.
   Methods: A structured questionnaire containing items based on constructs
   validated in the original UTAUT study was distributed to 661 Flemish
   caregivers. We performed a complete case analysis using data from 282
   questionnaires to obtain information regarding the effects of
   performance expectancy (PE), effort expectancy (EE), social influence
   (SI), facilitating conditions (FC), anxiety (ANX), self-efficacy (SE)
   and attitude towards using technology (ATUT) on behavioural intention
   (BI) to use the BelRAI web application.
   Results: The values of the internal consistency evaluation of each
   construct demonstrated adequate reliability of the survey instrument.
   Convergent and discriminant validity were established. However, the
   items of the ATUT construct cross-loaded on PE. FC proved to have the
   most significant influence on BI to use BelRAI, followed by SE. Other
   constructs (PE, EE, SI, ANX, ATUT) had no significant influence on BI.
   The `direct effects only' model explained 30.8\% of the variance in BI
   to use BelRAI.
   Conclusions: Critical factors in stimulating the behavioural intention
   to use new technology are good-quality software, interoperability and
   compatibility with other information systems, easy access to computers,
   training facilities, built-in and online help and ongoing IT support.
   These findings can be used by policy makers to maximise the acceptance
   and the success of new technology. For researchers, the conclusions of
   the original UTAUT study with regards to the item and scale construction
   should not be copied blindly across different information systems. A
   bottom-up approach is preferred when building upon the UTAUT model.}},
DOI = {{10.1186/1472-6947-13-129}},
Article-Number = {{129}},
EISSN = {{1472-6947}},
ResearcherID-Numbers = {{Declercq, Anja G./G-8014-2011}},
ORCID-Numbers = {{Declercq, Anja G./0000-0003-3136-124X}},
Unique-ID = {{ISI:000328892500001}},
}

@article{ ISI:000594128100004,
Author = {Roth, Jonathan and Martin, Amory and Miller, Clayton and Jain, Rishee K.},
Title = {{SynCity: Using open data to create a synthetic city of hourly building
   energy estimates by integrating data-driven and physics-based methods}},
Journal = {{APPLIED ENERGY}},
Year = {{2020}},
Volume = {{280}},
Month = {{DEC 15}},
Abstract = {{Cities officials are increasingly interested in understanding spatial
   and temporal energy patterns of the built environment to facilitate
   their city's transition to a low-carbon future. In this paper, a new
   Augmented-Urban Building Energy Model (A-UBEM) is proposed that combines
   data-driven and physics-based simulation methods to produce synthetic
   hourly load curve estimates for every building within a city-similar to
   data an hourly smart meter would measure. By using only publicly
   available data, a generalizable two-step process is implemented-that
   other cities with similar available data can replicate-using New York
   City as a case study. Step (1) estimates the annual energy use for every
   building in the city using supervised machine learning algorithms. Step
   (2) extends these results and leverages physics-based simulation models
   through a convex optimization formulation that minimizes the squared
   difference between the aggregated building demand and the observed
   city-wide hourly electricity demand. Results from step (1) show that the
   Random Forest algorithm performs best with a mean log squared error of
   0.293, while the convex optimization in step (2) results in a mean
   training error of 6.11\% mean absolute percentage error (MAPE). To
   validate the stability of the produced load curves, Monte Carlo
   simulations are conducted, using random subsets of buildings from the
   city , which produce an out-of-sample error averaging 6.41\% MAPE across
   each simulation . Particle swarm optimization is also explored-using the
   results from the Monte Carlo simulation-to assess if the model could be
   improved by relaxing certain constraints, but marginal error reductions
   are found, further proving the stability of the proposed model. Overall,
   A-UBEM is a first step towards creating highly granular urban-scale
   synthetic hourly load curves solely using open data. Such load curves
   are integral for planning sustainable cities and accelerating the
   adoption of low-carbon distributed energy resources (DERs) and district
   energy systems.}},
DOI = {{10.1016/j.apenergy.2020.115981}},
Article-Number = {{115981}},
ISSN = {{0306-2619}},
EISSN = {{1872-9118}},
ResearcherID-Numbers = {{Miller, Clayton/I-8813-2019}},
ORCID-Numbers = {{Miller, Clayton/0000-0002-1186-4299}},
Unique-ID = {{ISI:000594128100004}},
}
